{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "CUDA = False#torch.cuda.is_available()\n",
    "print(CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from lifelines.utils import concordance_index \n",
    "import sys\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import network\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_CHOICE = 1\n",
    "# WHAS = 1\n",
    "# GBSG = 2\n",
    "# METABRIC = 3\n",
    "# SUPPORT = 4\n",
    "\n",
    "if (DATASET_CHOICE == 1):\n",
    "    # WHAS\n",
    "    ds = pd.read_csv('./datasets/whas1638.csv',sep=',')\n",
    "    train = ds[:1310]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[1310:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5']].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5']].as_matrix() \n",
    "elif (DATASET_CHOICE == 2):\n",
    "    # GBSG\n",
    "    ds = pd.read_csv('./datasets/gbsg2232.csv',sep=',')\n",
    "    train = ds[:1546]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[1546:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5', '6']].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5', '6']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5', '6']].as_matrix() \n",
    "elif (DATASET_CHOICE == 3):\n",
    "    # for METABRIC\n",
    "    ds = pd.read_csv('./datasets/metabric1904.csv',sep=',')\n",
    "    train = ds[:1523]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[1523:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix() \n",
    "elif (DATASET_CHOICE == 4):\n",
    "    # for SUPPORT\n",
    "    ds = pd.read_csv('./datasets/support8873.csv',sep=',')\n",
    "    train = ds[:7098]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[7098:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', ]].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "x_train = scl.fit_transform(x_train)\n",
    "\n",
    "e_train = train['fstat']\n",
    "t_train = train['lenfol']\n",
    "\n",
    "\n",
    "x_valid = scl.fit_transform(x_valid)\n",
    "\n",
    "e_valid = valid['fstat']\n",
    "t_valid = valid['lenfol']\n",
    "\n",
    "\n",
    "x_test = scl.transform(x_test)\n",
    "\n",
    "e_test = test['fstat']\n",
    "t_test = test['lenfol']\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "e_train = torch.from_numpy(e_train.as_matrix()).float()\n",
    "t_train = torch.from_numpy(t_train.as_matrix())\n",
    "\n",
    "x_valid = torch.from_numpy(x_valid).float()\n",
    "e_valid = torch.from_numpy(e_valid.as_matrix()).float()\n",
    "t_valid = torch.from_numpy(t_valid.as_matrix())\n",
    "\n",
    "\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "e_test = torch.from_numpy(e_test.as_matrix()).float()\n",
    "t_test = torch.from_numpy(t_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1210, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1638, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>lenfol</th>\n",
       "      <th>fstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.44662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.589239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.64232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.48160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1453.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.58821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.12942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.616980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1    2         3    4    5       lenfol  fstat\n",
       "0  1.0  84.0  0.0  22.44662  1.0  1.0     7.589239      1\n",
       "1  0.0  36.0  0.0  31.64232  0.0  1.0  1266.000000      0\n",
       "2  0.0  76.0  0.0  46.48160  0.0  1.0  1453.000000      0\n",
       "3  0.0  47.0  1.0  27.58821  1.0  0.0   608.000000      0\n",
       "4  0.0  80.0  0.0  24.12942  1.0  0.0    22.616980      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    x_train = x_train.cuda()\n",
    "    e_train = e_train.cuda()\n",
    "    t_train = t_train.cuda()\n",
    "    x_valid = x_valid.cuda()\n",
    "    e_valid = e_valid.cuda()\n",
    "    t_valid = t_valid.cuda()\n",
    "    x_test = x_test.cuda()\n",
    "    e_test = e_test.cuda()\n",
    "    e_test = t_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting risk set computation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0d50b29441472ba432cc55e0f4e63a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af108fa31a74dbe8b92e34587f59617"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ee5fd5c270458f80b37e55a322a168"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "risk set computed\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "#         m.weight.data.fill_(0)\n",
    "#         m.bias.data.fill_(1)\n",
    "\n",
    "def init_weights_for_cox(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "t_ = t_train.cpu().data.numpy()\n",
    "        \n",
    "print(\"starting risk set computation...\")\n",
    "risk_set = []\n",
    "for i in tqdm(range(len(t_))):\n",
    "\n",
    "    risk_set.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "t_ = t_valid.cpu().data.numpy()\n",
    "        \n",
    "risk_set_valid = []\n",
    "for i in tqdm(range(len(t_))):\n",
    "\n",
    "    risk_set_valid.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "    \n",
    "    \n",
    "t_ = t_test.cpu().data.numpy()\n",
    "\n",
    "risk_set_test = []\n",
    "for i in tqdm(range(len(t_test))):\n",
    "\n",
    "    risk_set_test.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "print(\"risk set computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def elbo(risk, gated_output, E, risk_set, CUDA):\n",
    "#     lgo_sm = nn.LogSoftmax(dim=1)(gated_output)\n",
    "#     lnumerator = torch.mul(torch.exp(lgo_sm), risk)\n",
    "    \n",
    "#     lnumerator = torch.sum(lnumerator, dim=1)\n",
    "    \n",
    "#     expected_risks = risk + lgo_sm\n",
    "#     expected_risks = torch.logsumexp(expected_risks, dim=1)\n",
    "#     ldenominator = []\n",
    "#     for i in range(risk.shape[0]):\n",
    "#         ldenominator.append(torch.logsumexp(expected_risks[risk_set[i]], dim=0))\n",
    "        \n",
    "#     ldenominator = torch.stack(ldenominator, dim=0)\n",
    "#    # ldenominator = torch.log(ldenominator)\n",
    "    \n",
    "#     likelihoods = lnumerator - ldenominator\n",
    "    \n",
    "#     E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# #     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "#     likelihoods = likelihoods[E]\n",
    "#     neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "#     return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elbo(risk, gated_output, E, risk_set, CUDA):\n",
    "    go_sm = nn.Softmax(dim=1)(gated_output)\n",
    "    lnumerator = torch.mul(go_sm, risk)\n",
    "    \n",
    "    lnumerator = torch.sum(lnumerator, dim=1)\n",
    "    \n",
    "    expected_risks = torch.exp(risk) * go_sm\n",
    "    expected_risks = torch.sum(expected_risks, dim=1)\n",
    "    ldenominator = []\n",
    "    for i in range(risk.shape[0]):\n",
    "        ldenominator.append(torch.sum(expected_risks[risk_set[i]], dim=0))\n",
    "        \n",
    "    ldenominator = torch.stack(ldenominator, dim=0)\n",
    "    ldenominator = torch.log(ldenominator)\n",
    "    \n",
    "    likelihoods = lnumerator - ldenominator\n",
    "    \n",
    "    E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "#     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "    likelihoods = likelihoods[E]\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to change code so that test runs after model is trained\n",
    "def train(gated_network, betas_network, risk_set, x_train, e_train, t_train, risk_set_valid, x_valid, e_valid, t_valid, CUDA, \n",
    "          optimizer, n_epochs,x_test,e_test,t_test,risk_set_test):\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    # Initialize Metrics\n",
    "    c_index_soft = []\n",
    "    c_index_hard = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    test_loss = []\n",
    "    test_c_index_soft = []\n",
    "    test_c_index_hard = []\n",
    "    diff = 1e-4\n",
    "    prev_loss_train = 0\n",
    "    prev_loss_valid = 0\n",
    "    bad_cnt = 0\n",
    "    start = time.time()\n",
    "      \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        gated_network.train()\n",
    "        betas_network.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        gated_outputs = gated_network(x_train)\n",
    "        lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "        betas_output = betas_network(x_train)\n",
    "        \n",
    "        ci_train_soft,ci_train_hard = get_concordance_index(betas_output, gated_outputs, t_train, e_train)\n",
    "        c_index_soft.append(ci_train_soft)\n",
    "        c_index_hard.append(ci_train_hard)\n",
    "        \n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss = elbo(betas_output, gated_outputs, e_train, risk_set, CUDA) + (betas_network[0].weight**2).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        my_loss = loss.cpu().data.numpy()\n",
    "        train_loss.append(my_loss)\n",
    "        if abs(my_loss - prev_loss_train) < diff:\n",
    "            break\n",
    "        prev_loss_train = my_loss\n",
    "        \n",
    "        \n",
    "        torch.cuda.empty_cache()          \n",
    "        \n",
    "        \n",
    "        ################################################# Validation #######################################################\n",
    "        gated_network.eval()\n",
    "        betas_network.eval()\n",
    "\n",
    "        gated_outputs_valid = gated_network(x_valid)\n",
    "        lsoftmax_valid = nn.LogSoftmax(dim=1)(gated_outputs_valid)\n",
    "        \n",
    "        betas_output_valid = betas_network(x_valid)\n",
    "              \n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss_valid = elbo(betas_output_valid, gated_outputs_valid, e_valid, risk_set_valid, CUDA)\n",
    "        \n",
    "\n",
    "        my_loss_valid = loss_valid.cpu().data.numpy()\n",
    "        valid_loss.append(my_loss_valid)\n",
    "        if my_loss_valid - prev_loss_valid > diff:\n",
    "            bad_cnt+=1\n",
    "            #if bad_cnt>2:\n",
    "                #break\n",
    "        #else:\n",
    "            #bad_cnt=0\n",
    "        prev_loss_valid = my_loss_valid\n",
    "        \n",
    "        torch.cuda.empty_cache()          \n",
    "\n",
    "        \n",
    "        ################################################# Test #############################################################\n",
    "        gated_network.eval()\n",
    "        betas_network.eval()\n",
    "\n",
    "        \n",
    "        gated_outputs_test = gated_network(x_test)\n",
    "        lsoftmax_test = nn.LogSoftmax(dim=1)(gated_outputs_test)\n",
    "        \n",
    "        betas_output_test = betas_network(x_test)\n",
    "        \n",
    "        ci_test_soft,ci_test_hard = get_concordance_index(betas_output_test, gated_outputs_test, t_test, e_test)\n",
    "        test_c_index_soft.append(ci_test_soft)\n",
    "        test_c_index_hard.append(ci_test_hard)\n",
    "        \n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss_test = elbo(betas_output_test, gated_outputs_test, e_test, risk_set_test, CUDA)\n",
    "        \n",
    "\n",
    "        my_loss_test = loss_test.cpu().data.numpy()\n",
    "        test_loss.append(loss_test)\n",
    "        \n",
    "        \n",
    "        torch.cuda.empty_cache()          \n",
    "        #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "     \n",
    "        \n",
    "    metrics = {}\n",
    "    metrics['train_loss'] = train_loss\n",
    "    metrics['c-index-soft'] = c_index_soft\n",
    "    metrics['c-index-hard'] = c_index_hard\n",
    "    metrics['test_loss'] = test_loss\n",
    "    metrics['c-index-test-soft'] = test_c_index_soft\n",
    "    metrics['c-index-test-hard'] = test_c_index_hard\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elbo_batch(risk, gated_output, E, risk_set, CUDA):\n",
    "    \n",
    "\n",
    "    go_sm = nn.Softmax(dim=1)(gated_output)\n",
    "    lnumerator = torch.mul(go_sm[0], risk[0])\n",
    "\n",
    "#     lnumerator = torch.sum(lnumerator, dim=1)\n",
    "\n",
    "    expected_risks = torch.exp(risk) * go_sm\n",
    "    expected_risks = torch.sum(expected_risks, dim=1)\n",
    "\n",
    "    ldenominator = torch.sum(expected_risks, dim=0)\n",
    "\n",
    "    ldenominator = torch.log(ldenominator)\n",
    "\n",
    "    likelihoods = lnumerator - ldenominator\n",
    "\n",
    "\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "\n",
    "    \n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to fix for hard and soft gating\n",
    "def train_batch(gated_network, betas_network, risk_set, x, e, t, CUDA, optimizer, n_epochs,x_valid,e_valid,t_valid,risk_set_validation):\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    # Initialize Metrics\n",
    "    c_index = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    valid_c_index = []\n",
    "    diff = 1e-4\n",
    "    \n",
    "    prev_loss_train = 0\n",
    "    prev_loss_valid = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        eloss = 0\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            \n",
    "            if e[i] == 0:\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # print(\"x: \", x)\n",
    "            gated_outputs = gated_network(x[risk_set[i]])\n",
    "            lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "\n",
    "            betas_output = betas_network(x[risk_set[i]])\n",
    "            \n",
    "\n",
    "    #         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "            loss = elbo_batch(betas_output, gated_outputs, e[i], risk_set, CUDA)\n",
    "                        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            torch.cuda.empty_cache() \n",
    "            \n",
    "            eloss+=loss.cpu().data.numpy()\n",
    "\n",
    "        gated_outputs = gated_network(x)\n",
    "        lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "        \n",
    "        betas_output = betas_network(x)\n",
    "        \n",
    "        loss_train = elbo(betas_output, gated_outputs, e, risk_set, CUDA)\n",
    "\n",
    "        \n",
    "        ci_train = get_concordance_index(betas_output, gated_outputs, t, e)\n",
    "        c_index.append(ci_train)\n",
    "    \n",
    "            #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "\n",
    "        gated_outputs_valid = gated_network(x_valid)\n",
    "        lsoftmax_valid = nn.LogSoftmax(dim=1)(gated_outputs_valid)\n",
    "\n",
    "        betas_output_valid = betas_network(x_valid)\n",
    "\n",
    "        ci_valid = get_concordance_index(betas_output_valid, gated_outputs_valid, t_valid, e_valid)\n",
    "        valid_c_index.append(ci_valid)\n",
    "\n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss_valid = elbo(betas_output_valid, gated_outputs_valid, e_valid, risk_set_validation, CUDA)\n",
    "\n",
    "\n",
    "        my_loss_valid = loss_valid.cpu().data.numpy()\n",
    "        valid_loss.append(my_loss_valid)\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()          \n",
    "            #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "        \n",
    "        train_loss.append(eloss)\n",
    "        if abs(eloss - prev_loss) < diff:\n",
    "            break\n",
    "        prev_loss = eloss\n",
    "\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['train_loss'] = train_loss\n",
    "    metrics['c-index'] = c_index\n",
    "    metrics['valid_loss'] = valid_loss\n",
    "    metrics['c-index-valid'] = valid_c_index\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_log_likelihood(risk, lsoftmax, E, risk_set, CUDA):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "#     new_risk = []\n",
    "#     for i in range(len(risk_set)):\n",
    "#         new_risk.append(risk[risk_set[i]])\n",
    "        \n",
    "#     log_risk = []\n",
    "#     for i in range(len(new_risk)):\n",
    "#         temp = torch.logsumexp(new_risk[i], 0)\n",
    "#         log_risk.append(temp)\n",
    "\n",
    "    lnumerator = risk\n",
    "    \n",
    "    idxs = range(risk.shape[0])\n",
    "    \n",
    "    \n",
    "    ldenominator = []\n",
    "    \n",
    "    for i in range(len(idxs)):\n",
    "        ldenominator.append(torch.logsumexp(risk[risk_set[i]], dim=0))\n",
    "            \n",
    "                            \n",
    "    ldenominator = torch.stack(ldenominator, dim=0)\n",
    "    print(ldenominator.shape)\n",
    "#     print(lnumerator.shape)\n",
    "    \n",
    "    \n",
    "    likelihoods = lnumerator - ldenominator\n",
    "    print(likelihoods.shape)\n",
    "    \n",
    "    E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "#     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "    likelihoods = likelihoods[E] + lsoftmax[E]\n",
    "    likelihoods = torch.logsumexp(likelihoods, dim=1)\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "\n",
    "    return neg_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_concordance_index(x, gated_x, t, e, **kwargs):\n",
    "#     x = x.detach().cpu().numpy()\n",
    "    t = t.detach().cpu().numpy()\n",
    "    e = e.detach().cpu().numpy()\n",
    "    softmax = torch.nn.Softmax(dim=1)(gated_x)\n",
    "    \n",
    "    r = x.shape[0]\n",
    "    \n",
    "    soft_computed_hazard = torch.exp(x)\n",
    "    hard_computed_hazard = soft_computed_hazard[range(r),gated_x.argmax(1)[1]]\n",
    "\n",
    "    \n",
    "    soft_computed_hazard = torch.mul(softmax, soft_computed_hazard)\n",
    "    soft_computed_hazard = torch.sum(soft_computed_hazard, dim = 1)\n",
    "    \n",
    "    \n",
    "    return concordance_index(t,-1*soft_computed_hazard.detach().cpu().numpy(),e),concordance_index(t,-1*hard_computed_hazard.detach().cpu().numpy(),e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806a9d0ca68c4b758cd32d757839facf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "n_in = x_train.shape[1]\n",
    "k = 5\n",
    "\n",
    "betas_network = nn.Sequential(nn.Linear(n_in, k, bias=False) )\n",
    "#betas_network.apply(init_weights)\n",
    "\n",
    "# Construct Neural Network\n",
    "layers_sizes = [n_in, k]\n",
    "layers = []\n",
    "for i in range(len(layers_sizes)-2):\n",
    "    layers.append(nn.Linear(layers_sizes[i],layers_sizes[i+1], ))\n",
    "    layers.append(nn.ReLU())\n",
    "\n",
    "layers.append(nn.Linear(layers_sizes[-2], layers_sizes[-1], bias=False))\n",
    "gated_network = nn.Sequential(*layers)\n",
    "#gated_network.apply(init_weights)\n",
    "\n",
    "#optimizer = optimizer = torch.optim.SGD(my_network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=L2_reg, nesterov=True)\n",
    "optimizer = torch.optim.Adam(list(gated_network.parameters()) + list(betas_network.parameters()), lr=1e-3)\n",
    "betas_network.train()\n",
    "gated_network.train()\n",
    "\n",
    "if CUDA:\n",
    "    gated_network.cuda()\n",
    "    betas_network.cuda()\n",
    "\n",
    "# If you have validation data, you can add it as the valid_dataloader parameter to the function\n",
    "n_epochs = 1000\n",
    "metrics = train(gated_network, betas_network, risk_set, x_train, e_train, t_train,  risk_set_valid, x_valid, e_valid, t_valid, CUDA, optimizer, n_epochs,x_test,e_test,t_test,risk_set_test)\n",
    "print() \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8367793509092919\n",
      "0.8013679285828309\n",
      "0.8256092217466042\n",
      "0.8070713539384543\n"
     ]
    }
   ],
   "source": [
    "print(metrics['c-index-soft'][-1])\n",
    "print(metrics['c-index-hard'][-1])\n",
    "print(metrics['c-index-test-soft'][-1])\n",
    "print(metrics['c-index-test-hard'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8604f119de224289b0d87aa2623a2be9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 2\n",
      "Learning Rate: 0.0001\n",
      "0.7459115635399143\n",
      "0.6869138106757371\n",
      "0.6102711693359996\n",
      "0.5792857276346035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823addd6f7ee472ebd31d6ff7e4e777e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 2\n",
      "Learning Rate: 0.001\n",
      "0.8218868573252944\n",
      "0.8218721220084237\n",
      "0.6411071034779195\n",
      "0.6411257919228541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de3eed28bc74c79a5e6b0658cacb11e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 5\n",
      "Learning Rate: 0.0001\n",
      "0.8125962400383119\n",
      "0.7965003622432064\n",
      "0.6391821936496664\n",
      "0.6341550019622867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25889b9378b8425b8736612d1a51f13d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 5\n",
      "Learning Rate: 0.001\n",
      "0.835163377825804\n",
      "0.8182791605781156\n",
      "0.6565250705488797\n",
      "0.6154478685828553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b618c6631ebc4a5fbd05133b1506ebdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 7\n",
      "Learning Rate: 0.0001\n",
      "0.7932266660117637\n",
      "0.7845868585532375\n",
      "0.6277261769048198\n",
      "0.6313891121119811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637ea0c8b3a9404da4a185e0ee903375"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models: 7\n",
      "Learning Rate: 0.001\n",
      "0.8302172231295357\n",
      "0.7970283777644068\n",
      "0.6460221644956923\n",
      "0.6318002579005401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdeb72ae5214454ca3f8a7e20c34ee6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models: 10\n",
      "Learning Rate: 0.0001\n",
      "0.8015226494099733\n",
      "0.7929589744219459\n",
      "0.6373507260460857\n",
      "0.6374254798258236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe314919d1442f4b205458c9d14fe3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models: 10\n",
      "Learning Rate: 0.001\n",
      "0.8350233923155322\n",
      "0.7991551751660793\n",
      "0.6519837784297968\n",
      "0.6341923788521557\n",
      "Best Concordance for hard gating: 0.6411257919228541 with parameters:\n",
      "Learning rate- 0.001\n",
      "Linear models- 2\n",
      "Best Concordance for hard gating: 0.6565250705488797 with parameters:\n",
      "Learning rate- 0.001\n",
      "Linear models- 5\n"
     ]
    }
   ],
   "source": [
    "#grid search:\n",
    "\n",
    "linear_models=[2,5,7,10]\n",
    "learning_rates=[1e-4,1e-3]\n",
    "best_k_hard=0\n",
    "best_lr_hard=0\n",
    "best_k_soft=0\n",
    "best_lr_soft=0\n",
    "\n",
    "best_ci_soft=0\n",
    "best_ci_hard=0\n",
    "\n",
    "for k in linear_models:\n",
    "    for lr in learning_rates:\n",
    "        n_in = x_train.shape[1]\n",
    "        betas_network = nn.Sequential(nn.Linear(n_in, k, bias=False) )\n",
    "        betas_network.apply(init_weights)\n",
    "\n",
    "    # Construct Neural Network\n",
    "        layers_sizes = [n_in,20,20, k]\n",
    "        layers = []\n",
    "        for i in range(len(layers_sizes)-2):\n",
    "            layers.append(nn.Linear(layers_sizes[i],layers_sizes[i+1], ))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(layers_sizes[-2], layers_sizes[-1], bias=False))\n",
    "        gated_network = nn.Sequential(*layers)\n",
    "        gated_network.apply(init_weights)\n",
    "\n",
    "    #optimizer = optimizer = torch.optim.SGD(my_network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=L2_reg, nesterov=True)\n",
    "        optimizer = torch.optim.Adam(list(gated_network.parameters()) + list(betas_network.parameters()), lr=lr)\n",
    "    \n",
    "        if CUDA:\n",
    "            gated_network.cuda()\n",
    "            betas_network.cuda()\n",
    "\n",
    "    # If you have validation data, you can add it as the valid_dataloader parameter to the function\n",
    "        n_epochs = 1000\n",
    "        metrics = train(gated_network, betas_network, risk_set, x_train, e_train, t_train,  risk_set_valid, x_valid, e_valid, t_valid, CUDA, optimizer, n_epochs,x_test,e_test,t_test,risk_set_test)\n",
    "        print() \n",
    "        print(\"Models:\",k)\n",
    "        print(\"Learning Rate:\",lr)\n",
    "    \n",
    "        print(metrics['c-index-soft'][-1])\n",
    "        print(metrics['c-index-hard'][-1])\n",
    "        print(metrics['c-index-test-soft'][-1])\n",
    "        print(metrics['c-index-test-hard'][-1])\n",
    "    \n",
    "        if metrics['c-index-test-soft'][-1]>best_ci_soft:\n",
    "            best_ci_soft=metrics['c-index-test-soft'][-1]\n",
    "            best_k_soft=k\n",
    "            best_lr_soft=lr\n",
    "\n",
    "        if metrics['c-index-test-hard'][-1]>best_ci_hard:\n",
    "            best_ci_hard=metrics['c-index-test-hard'][-1]\n",
    "            best_k_hard=k\n",
    "            best_lr_hard=lr\n",
    "        \n",
    "print('Best Concordance for hard gating:',best_ci_hard, \"with parameters:\" )\n",
    "print('Learning rate-',best_lr_hard)\n",
    "print('Linear models-',best_k_hard)\n",
    "\n",
    "print('Best Concordance for hard gating:',best_ci_soft, \"with parameters:\" )\n",
    "print('Learning rate-',best_lr_soft)\n",
    "print('Linear models-',best_k_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7847247969260844\n",
      "0.5149482624962082\n"
     ]
    }
   ],
   "source": [
    "print(np.max(metrics['c-index-valid-soft']))\n",
    "print(np.max(metrics['c-index-valid-hard']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of epochs:  1000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOZdjnjycc87l2P8Dquhq9t3xaTsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VfX9+PHXOwnZGxL23oICEgHF\nAQ5Eq+KoA7WiUmmtVqu2/rTDWUf71apt7UAENzjqQEURFTcrCAiEvUMgCUnI3vf9++OcaMRALnhv\n7s3N+/l43EfuOfeM98mB+875TFFVjDHGmEMJC3QAxhhjgp8lC2OMMc2yZGGMMaZZliyMMcY0y5KF\nMcaYZlmyMMYY0yxLFsYYY5plycIYY0yzLFkYY4xpVkSgA/CVDh06aK9evQIdhjHGtCrLly/fp6pp\nzW0XMsmiV69eZGZmBjoMY4xpVURkhzfbWTGUMcaYZlmyMMYY0yxLFsYYY5plycIYY0yzLFkYY4xp\nliULY4wxzbJkYYwxplkh08/CGGNCXWlVLTsLK8grqSa/tJqSqloqa+ppHx/F5aN7+PXcliyMMSYI\neDxKbmkV2UWV5OyvpLC8ho25ZeSXVlFTr2zcW8rekqom9z22R3LrThYiMhF4AggHZqjqwwd83gN4\nFkh2t7lDVee5n90JTAXqgZtUdb4/YzXGGF+rq/dQVeehqraejbmllFTWUVlbR0llHTnFleSVVJNb\nUkVuSRV7iquoqKn/3v4pse3omBhNu/Awju2ZzJAuSfTuEEfHxGjSE6JIim1HbLtwIsL9X6Pgt2Qh\nIuHAk8AZQDawTETmqmpWo83+CLyiqv8WkaOAeUAv9/1lwBCgC/ChiAxQ1e//Jo0xJkDKquvILaki\nr8QpDiqtqqOwvJrdRZVsL6hgX1k16/eWUu/RJvePDA8jPTGKjonRDOiYwMkD0uibFk+3lBi6JseQ\nFNuOtPgoRKSFr6xp/nyyGAVsVtWtACIyB5gENE4WCiS675OAHPf9JGCOqlYD20Rks3u8RX6M1xhj\nvqWq7CmuYmdhBbvc1/aCCrbtK2dnYQXFlbVN7pcQFUHPDrGkJ0RxXK9UOidFExkRRq8OcaQnRBEX\nGUFcVAQd4iODJhF4w5/Joiuwq9FyNjD6gG3uAT4QkV8DccDpjfZdfMC+Xf0TpjGmLaqoqSNnfxV7\niivZs7+KnEY/c/ZX/qBYKEygS3IMvTvEMax7Z7qlxNLJLQ5KjGlHYnQ7kuPakRAV0aqSgLf8mSya\n+m0d+Dw2GXhGVR8VkeOB50VkqJf7IiLTgGkAPXr4t3LHGNO61HuUoooaNuwtZWNuKZvzyiirrmNr\nfjm7iirYX/HDJ4O0hCi6JEXTP90pFuqTFk+v9rH0SI2lS3IM7VqgbiBY+TNZZAPdGy1347tipgZT\ngYkAqrpIRKKBDl7ui6pOB6YDZGRkNF0waIwJWfUeZXtBORv2lrK9oJzsokrnVVjBjsKK79UXJMW0\nIzm2HV2TYzjnmM50TnLqBjonRdMlOYaOiU5xkWmaP5PFMqC/iPQGduNUWF9+wDY7gdOAZ0RkMBAN\n5ANzgZdE5G84Fdz9gaV+jNUYE6RUlZziKkqratmcV8aWvHI255exJa+MLfllVNd5vt02NS6Sbikx\nDOqcwJlDO5EWH8XATgn0T48nLSF4KotbI78lC1WtE5Ebgfk4zWJnqupaEbkPyFTVucBtwFMicgtO\nMdPVqqrAWhF5BacyvA64wVpCGROaPB6lpKqWoopaCstryC2pYtu+ctbvLWVTbim5JVUUNSoyEoGu\nyTH0S4/nhL7tGdgpgUGdEumTFkdclHUd8xdxvptbv4yMDLWZ8owJbvvKqlm2rZD9lbVsLygnK6eE\ndXtK2FdW84Ntu6XEMLBjAumJ0QzoGE/7+Cj6psXRp0M8MZHhAYg+NInIclXNaG47S8PGGJ8qq65j\n3Z4S1u4uZm1OCbuKKigoq2FfWfX3nhAiw8MY0Cme0b3bM6x7Eh3io0iNiyQ9IZruqTEkRLcL4FWY\nA1myMMYckYKyarL2lLAlr4z9lbVsyisjK6eE7QXlNBRYtI+LpE9aHH3T4jmudyo9U2PJ6JVKekKU\nVSi3MpYsjDGHpKpsyXdaHG3Kc5qgbswtZWNu2fe2654aw5DOSVwwoitDuiQypEsSHROtUjlUWLIw\nxnxPaVUt32QXs2RbIVk5JXy9s4jCcqdOQQS6p8TSLz2eScO7MqJHMv3S40mMbkd0O6tHCGWWLIxp\nw6pq61m/t5Rvsvezctd+vskuZkt+GapOj+VeHeI4dVA6x/VKYUiXJPqmWeVyW2XJwpg2Iq+0iu37\nKtiaX8YXm/exctd+9hRXfdtxrUN8FMO6JXHesC4c0y2JkT1TrJLZfMuShTEhqLiyljW7i1m3p4QV\nO52nht37K7/9PC0hilG9Ujl/eBxDuiQyrHsynZOirX7BHJQlC2NCRGlVLe98s4f31uxl8dYCatye\nzV2TYxjRI5lrxvaif8cEuqXE0KdDnCUGc1gsWRjTiu0qrGDuqhw+35RP5vYi6jxKnw5xXDWmJ2P7\ndWBIl0TSE6MDHabxhip46qByP1QWwrbPnFdtJdRVQele5xUV72yb0BEiE6CmDDoMgIue8mt4liyM\naWXySqpYtLWAt1ft4cN1uQAM6ZLIdSf34bRB6YzsmWJPDcFAFfLXw75NsHs5FGyG6lKQMAiLABTU\n4ySIou1QkuO8byyhi5MUImIgbSB0PRY89dAuGsryoLoM4tIgsYvfL8eShTGtQE2dh4Ub8nh+0Q6+\n2rIPj0JybDt+c3p/LhzRjR7tYwMdYtvjqYd9G6EsF8ryoTQHdi2FqmIo2e18mdc09EURSBsEUQlO\nglCPkzQkzGmP3PFo6DMeErtCbCrEpEByD+h2nPN5ELBkYUyQyi6q4H/Ld/PZpnxWZxdTU++ha3IM\nN47vxxlHdWJw54QWmXu5Taivg4p9zhd8WR6U50F5vvM0oB6orXKeCqpLne3K86Gy6IfHiUuD9v0h\n/SjoPwE6HQ0dh0JqH4hO/OH2rYglC2OCRGVNPYu27uOTDfl8siGfnYUVAIzokcyUE3oysmcKpw/u\naAnCW9Vlzl/4xbugaIdT1JOzAqr2O3/9x6RCfY2THCoKaGJ+NQhrB7HtnfcxKc779MEQdzJExkP7\nvpDSG+I7QnwaRCcHzZOAr1myMCaAqmrrWbKtkI/X5fJy5i6qaj3EtAtnbL/2XDO2F+MGptO7Q1yg\nwwx+FYVO3UDBZue140vYteT724S1c4qCkrpCh4HOk0G7GOg++rsv+7j0797HdnA+D7e+JmDJwpgW\nV1fvIXNHEe9+s4c3V+6mtKoOEbhgeFcuOLYro3qnEhVhvaS/VVkEe9dAZBzUVTtPBbUVULQNdi6G\n4t2Qt/a77cMinKQw7k5I7QvJ3Z3y//hOEGZPZUfKkoUxLUBVWb+3lM825vP0F9vIK60mMiKMs4d2\n4vwRXa23tKfeaSJaWwk5X0PpHqfYaP9O2PA+1JY3vV9Sd0jtDafd5VQSt+8LyT0h3L7afM1+o8b4\nSV5JFe+v3cuOggo+WpfL9gKnDmJkzxTuOW8IJw9IIz7UZnZTdeoJGpqBhkVA7lonCUREQk2FU0kc\nk+xUEudvcFoUlez5YUJoqC8YdDYcfbHzVBEVD1FJEBnr1BkkdQvZOoJgE2L/Uo0JrKraej5en8fc\nlTl8uC6XOo8SGR7G6D6p/OKUvowbmEbnpJhAh+l7lUWw6mVY8TzkrvFun7AIaN/PaTnU9zQngbSL\ndSqQ0wY6Tw1hVhwXLCxZGPMj1dR5+HxTPm+vymFBVi7lNfV0iI/i6hN6MXl0D3q3jyMsLAT/+q0o\nhOWzYPPHkL0M6quhywg48yEnCWi9kxDi050v/uJdTj+CyHioq3SeEKwOodWwZGHMEaj3KIu3FvD2\nqhzeW7OX4spakmLacd7wLpx7TBdG92lPeCglCI8H8tc5Fcr5G2D9u1CS7XyWNtgpKjr+19Bt5MGP\nEZv63ft2NgRJa2PJwhgvVdXWs2hLAZ9syOPd1XvZV1ZNXGQ4E4Z04txhnTmxX1roTBNanA3bv3SK\nlXJWQE053+uH0PsUOPYq6Dseuo8KWJim5ViyMOYQVJVFWwqYs2wXH61zipiiIsI4bXA65x7ThfGD\n0kNjhri6GqeYaN3bsPF92LnIWR/fCUZc6TZHdesRep9sdQltkCULY5pQXFnLe6v38HLmLlbs3E9K\nrFPENHFoZ0b1Sm39s8WpOq2Stn0G2z6F1a9+N3xFWDs44SYYcr7THDUiMrCxmqBgycIYl6qydFsh\nc5btYt7qPVTXeeiTFsf9k4ZwcUb31v8EsW8zZL0B+3c5yaHWacpLeJRT59DpaBh4ttMcNSohsLGa\noGPJwhhgdXYx97+bxdJthSRER3BxRjcuyejO0V2TWudw37WVzgiou5c7TVlz1zr9GdTjFCn1PsUZ\n1mLgROgzDqKTAh2xCXKWLEybVVlTzzvf5DB/7V4+Wp9H+7hI7j1vCJdkdG+dxUz7d8Lq1yDrLSc5\neGqd9Uk9oOMQOGoSDL/C7b8QIhXxpsVYsjBtTmlVLa9kZjP9sy3kllTTKTGaG8b1Y9opfUhsbUNu\n1NXAzq9g5WxY/Yrz5NBtFJxwI/Q43mmpFJMS6ChNCLBkYdqM7fvKeXf1HmZ8vpWiilqO65XC45eO\nYEyf1NZT1OTxQOFW2LzAGVV16ydOxXRUIoy8BsbeBCm9Ah2lCUGWLExIU1Xmr83lsQUb2ZBbCsBJ\n/Ttw24SBDO+eHODoDkPhNlg1B9b8Dwo2OesSOjt1D0df7PR3iLShzI3/WLIwISsrp4T73lnL4q2F\n9E+P5+5zj+K0QR1bzxSk9bWw+UOn78Pq15zhNLodBxMegL6nOmMotZYnItPqWbIwIae4opZnvtrO\nPz7eREJ0BPefP5TJx3VvHTPM1dfB/h2waQEsnQ6FW5yxlIZdCmNugPRBgY7QtFGWLEzIqK33MPOL\nbTy6YCM1dR7OOaYzfz5/KMmxraBTWU258/Tw2SNQvNNZF50M5zzutGCyjnEmwCxZmJCwZGsBd721\nlg25pZw+OJ3fnD6AoV2DvO+AqjMF6No3nDGY9u90Zng79+/O6K2djwl0hMZ8y5KFadXySqt4aN56\n3lixm67JMUz/2UjOOKpjcLduUoWN8+GjeyEvCxBnHuifPOZUVNu4SyYIWbIwrdLyHYXMXrqLt1fl\n4FHlhvF9uXF8/+DuTOeph80fwcf3w95vIKELnP0IDPoJJHYJdHTGHJIlC9OqVNXW86c31/Dq8mxi\nI8P56chuTD2xN33S4gMdWtM8HqeIad3bkL0UqoqdHtWn/skZrM/qIkwr4ddkISITgSeAcGCGqj58\nwOePAePdxVggXVWT3c/qgdXuZztV9Tx/xmqCW229h3mr9/D3jzaxJb+cG8f34/pxfYkL1jmsq0ud\nYTeWToc9q5xOcwPPcnpVD70IohMDHaExh8Vv/9NEJBx4EjgDyAaWichcVc1q2EZVb2m0/a+BEY0O\nUamqw/0Vn2kdGua0/ufHm8naU0LP9rHMuCqD04/qGOjQmqYKa1+H+X+E0hxI6Q0/+RsMueD7M8UZ\n08r488+yUcBmVd0KICJzgElA1kG2nwzc7cd4TCuzZncxt76yko25ZXRJiuaJy4Zz9tGdaRes/SXy\n1sMHf3SG4kjpDVPehl4nWcc5ExL8mSy6ArsaLWcDo5vaUER6Ar2BjxutjhaRTKAOeFhV3/RXoCa4\nVNXW89C8dTy3eAcJURH858qRnDY4PTiThCpkZ8KSfzvFTuFRMPEvMOo6a9VkQoo/k0VTf05pE+sA\nLgNeU9X6Rut6qGqOiPQBPhaR1aq65XsnEJkGTAPo0aOHL2I2AZaVU8L1Ly5nR0EFV5/Qi1vOGEBS\nTJCOBFuwBebeBDu+cOaDyLgWTr4d4tMCHZkxPufPZJENdG+03A3IOci2lwE3NF6hqjnuz60i8glO\nfcaWA7aZDkwHyMjIOFgiMq1AUXkNLy7Zwb8+2UJ8VAQv/Xw0J/TrEOiwfsjjgb2rIHMmrHjRmUjo\n5NudIcFtAiETwvyZLJYB/UWkN7AbJyFcfuBGIjIQSAEWNVqXAlSoarWIdADGAn/1Y6wmQDwe5blF\n23nsw00UV9Zy+uB07jlvCN1SgmiwP1Uo3QvZy2DJf50nCQn77kkiIUgr243xoWaThYikq2reAesG\nquqGQ+2nqnUiciMwH6fp7ExVXSsi9wGZqjrX3XQyMEdVGz8ZDAb+KyIeIAynzuJgFeOmlVqytYB7\n384ia08JJ/XvwJ1nDeaoLkHUpDRvHXz+KGz6wOkfARARDWc+5DR/tSRh2hD5/nd0ExuIbAD+pKqv\nuMu3AVNV9agWiM9rGRkZmpmZGegwjJfmrsrhtldW0ikpmlvPGMD5w7sGxxAd1aWwdw18+rAzsVBk\nAhx1HnQcCj1GQ2ofm3nOhBQRWa6qGc1t500x1DhguohcDHQE1uE0izXmsJVW1fKHN9Ywd1UOo3qn\n8tRVGcFTgV1dBk+fCXlrISIGxv8BRvwMEjsHOjJjAq7ZZKGqe0TkfeBOwAPcqaplfo/MhJzy6jqm\nPbecZdsLuenUfvxqfD+i2wVJ89KVL8GCu6A830kQ4+6EpK6BjsqYoOFNncUCYA8wFKdF00wR+UxV\nf+vv4EzoyCup4qqZS1m/t5RHLx7GRSO7BTokh6cevnjMGdwvbRBcNAP6jAt0VMYEHW+KoZ5s1CFu\nv4icgPOUYYxXtu8r56qZS9lXVs2sa45j/MD0QIfkKMmB//0cdnzpTFN62UvQLibQURkTlLwphnrT\n7WHdX1U/BNoBj/s9MhMSFq7P4zcvr0QEXvz5aEb0CILK4dJcePtm2PieUzcx6V8w/HIblsOYQ/Cm\nGOo6nF7SqUBfnKKo/wCn+Tc005rV1Xu4750snlu0g0GdEpj+swx6tA+CvhNVJfDceVC0HUZfDyOu\nhE5DAx2VMUHPm2KoG3BaPy0BUNVNIhIk5QgmGO3eX8ktL69k6bZCrhnbi/83cVBwVGTnrIQ3fulM\nZXrZbBgwIdARGdNqeJMsqlW1pqENvIhEcPAxnkwblldSxb3vZPH+mr1ERYTx2KXDuGBEEFRk710N\nS/4DK2c7fSSufB36nBLoqIxpVbxJFp+KyO+BGBE5A/gV8LZ/wzKtzbo9JVw9aykllXVMPbE3PxvT\nk+6pAS52UoWVL8J7d0BNKQw4Cy74t3WqM+YIeJMs7gCm4sxa9wtgHjDDn0GZ1uWjdbn85uWVxEVG\n8L/rTwiOITtqq2D+7yHzaaf39UUznKaxVoltzBHxpjWUB3jKfRnzraraeu59O4vZS3cyuHMiM6Zk\n0DU5CJqe7t8Jb/8GtnwEw6+E8/4BYUE4F4YxrchBk4WIrOYQdROqeoxfIjKtwt7iKn7xwnJW7drP\nL07pw61nDCAqIggqsRf9Cz68G+pr4exHnEmIjDE/2qGeLM5xfzbMM/G8+/MKoMJvEZmgl7m9kF++\n8DWVNXX858qRTBzaKdAhOb78Oyz4Ewz8CZxxH3ToF+iIjAkZB00WqroDQETGqurYRh/dISJfAvf5\nOzgTfF5csoN75q6la3IML103mgEdEwIdkmPZ006iGHIhXPgUhPtzqhZj2h5v/kfFiciJqvoFgDvc\nR5x/wzLBpqbOw91z1zJ76U5OGZDG3y8bQVJsEIwWu3MJfPZX2Pwh9DrJqci2ua+N8TlvksVUnMED\nG+aM3A9c67+QTLApr67j589msmhrAdeP68tvJwwkPCzArYrWv+vMWrftU4jtAKf+CY6/wRKFMX7i\nTWuo5cAwEUnEmSyp2P9hmWCxr6yaqc8sY01OCX+7ZBgXHhvgTnbVpc5Q4pkzIak7nHIHjL0JIu1h\n1xh/8mZsqCjgIqAXENHQk1tVrc4ixO0sqOBnM5eQW1LFf68cyelHBXga0epSeG4S7F4OGVNh4kMQ\nERXYmIxpI7wphnoLKAaWA9X+DccEi237ypk8fTFVdfW8dN0Yjg30aLGFW2H2ZNi3CS55Do6aFNh4\njGljvEkW3VR1ot8jMUFjc14Zlz+1mDqPMvu6MQzuHOAe2Tkr4cWfgqcOJs+GAWcGNh5j2iBvksVX\nInK0qq72ezQm4DbmlnL5U0sAmDNtTOCaxno8sO0TWP6sU5md0MkZADBtQGDiMaaN8yZZnAhcLSLb\ncIqhBFDrwR161u0p4coZSwgPE166bgz90uNbPoj6WtjxldMLO2eFM+jfcVPhpNsg3kbGNyZQvEkW\nZ/k9ChNwX+8sYuozy4iKCGf2tDH07hCA1kUrXoD3/h/UlDkz2J3zGBxzGUQGwaRJxrRxhxobKlFV\nS4DSFozHBMDyHUVMfmoxnRKjeX7qKHq2b+FEoeo0hX3vdmdk2JNuhf5nQlQAnmyMMU061JPFSzjj\nQy3HGVCwcS8sBfr4MS7TQpZtL+TaWcvomBjFmzeMJTUusuWDWPIfeP8O6HE8TJ4DMcktH4Mx5pAO\nNTbUOe7P3i0XjmlJCzfkcf0Ly+mSFMMLPx/d8omirgYW/RM+uhcGTHSmOrWhxI0JSjbaWhs1f+1e\nfvXi1wzsmMBzU0fRIb6FO7eVF8AzZ0P+ehh0Dlz0tCUKY4KYJYs26IO1e7lp9gqO7prECz8fTXxU\nC/8zqKuGl6+Awm1w6Ysw6Cc2g50xQc6SRRuzfEcRN7z0NX3T4nl6SkbLJ4r6Oph7E+xcBD+dBYPP\naX4fY0zAefXcLyInisg17vs0EbF6jFZob3EV18xaSuekGOZMG0P7li56qi6D2ZfCN3PgpN/C0Atb\n9vzGmCPmzUCCdwMZwEBgFtAOeAEYe6j9THDJLqrgihlLqPMoM6/OIDm2hSuzq0vh2XNhzzfwk79B\nho1yb0xr4k0ZxAXACOBrAFXNEZEgmR7NeGNTbilTZi6ltLqOF34+mn7pLXz7inbAnMshd41TR2FF\nT8a0Ot4kixpVVRFRABGxiQNakfzSaq6etYxad1DAoV2Tmt/Jl/augRcugrpKmPQvSxTGtFLe1Fm8\nIiL/BZJF5DrgQ+Ap/4ZlfKG0qparZy2loLyaGVdltHyiWDkbZpwGEgbXvA8jrmjZ8xtjfMabmfIe\nEZEzgBKceou7VHWB3yMzP0pVbT3XPZfJhr2lPDUlg2HdW7BXdH0tzPstLH8Gep8MF/wXEru03PmN\nMT7nTQV3b+DzhgQhIjEi0ktVt/s7OHNk6j3KzXNWsHhrIY9fOpzxA1twtNbKInjxEshe6iSKy16C\nKKviMqa186YY6lXA02i53l3XLBGZKCIbRGSziNzRxOePichK97VRRPY3+myKiGxyX1O8OZ8Bj0f5\nwxurmb82l7vPPYrzR3RtmRPX18KKF+HfYyHnazj/PzDlbUsUxoQIbyq4I1S1pmFBVWtEpNl2lyIS\nDjwJnAFkA8tEZK6qZjU61i2Ntv81TqsrRCQVaGiyq8Byd98i7y6rbfJ4lD+8uZo5y3bx61P7cc3Y\nFuoOU1MBL10C2z+H9v1h6gfQdWTLnNsY0yK8ebLIF5HzGhZEZBKwz4v9RgGbVXWrm2zmAIeaOHky\nMNt9fyawQFUL3QSxALCpXQ9BVbn37bXMXrqLG8b35dYzWmhGOVVnaPHtn8PZj8CvFluiMCYEefNk\n8UvgRRH5J84w5buAq7zYr6u7bYNsYHRTG4pIT6A38PEh9v1BeYqITAOmAfTo0cOLkELX019s49lF\nO5h2ch9+O2Eg0hJjLXk8MO82WPE8nHgLjLrO/+c0xgSEN62htgBjRCQeEFX1djKkpr6t9CDbXga8\npqr1h7Ovqk4HpgNkZGQc7Nghb0FWLg/OW8dZQztxx8RBLZMoKvfDm9fDhnlOojjtbv+f0xgTMN60\nhooCLgJ6ARENX0Sqel8zu2YD3RstdwNyDrLtZcANB+w77oB9P2ku1rZodXYxN81ewdCuSTx6yTDC\nwlogUexe7rR4qtgHp/7JmR/bRo01JqR5Uwz1FlCMM2Ne9WEcexnQ3216uxsnIVx+4EYiMhBIARY1\nWj0feFBEUtzlCcCdh3HuNmHlrv1c+8wyUuMimTElg9hIP48gqwqfPwKf/h/EpMDUD6H7cf49pzEm\nKHjz7dJNVQ+7cllV60TkRpwv/nBgpqquFZH7gExVnetuOhmYo6raaN9CEbkfJ+EA3KeqhYcbQygr\nr67jxpe+pl248Mw1x5GeEO3fE9bVwMf3w1d/h6MmwVn/Bwkd/XtOY0zQ8CZZfCUiR6vq6sM9uKrO\nA+YdsO6uA5bvOci+M4GZh3vOtuLh99aze38lr/7iePp39HNfhtoqmHWW03/i2ClwzuM2q50xbYw3\nyeJE4GoR2YZTDCWAquoxfo3MHNR7q/fw/OIdTD2xNxm9Uv1/woUPOIniwhlwzMX+P58xJuh4kyzO\n8nsUxms7Cyr43WvfMKJHMrdPHOj/E371D6foaeTVliiMacO8aTq7A0BE0gE/F4ybQykoq+byGYsR\n4B+TRxAVEe6/k3nq4bNH4JMH4ajznQ53xpg2y5ums+cBjwJdgDygJ7AOGOLf0ExjHo9y5+urySut\n5uVpY+iWEuu/k5XscXplr5sLQy+CC6ZDuE3Xbkxb5k0t5f3AGGCjqvYGTgO+9GtU5gcemLeOD7Jy\nue2MAYzokdL8Dkdq/y54+gyns91pd8NFT1uiMMZ4VWdRq6oFIhImImGqulBE/uL3yMy3nl+8g6e/\n2MbVJ/Ri2sl9/Heigi1Oq6fqMvjZm9D7JP+dyxjTqniTLPa7Q318hjNGVB5Q59+wTIPlO4q4Z+5a\nTh2Uzp/OOcp/Q3kUbYdnz3XqKq77GNIH+ec8xphWyZtiqElAJXAL8D6wBTjXn0EZR1VtPb97bRWd\nEqN54rLhhPtrKI/i3U6iqK2Aq96yRGGM+QFvWkOVN1p81o+xmEZUnQrtrfnlvDB1NAnR7fxzIk89\nvHsrlO+Dq9+FTkP9cx5jTKt20GQhIqUcfJRYVDXRLxEZVJUH563jjRW7ue2MAZzYv4O/TgTv3wEb\n34cJf4aux/rnPMaYVu+gyUJVEwDcsZz2As/j9N6+ArC5Mv3oma+289Tn25hyfE9uPLWf/06U+TQs\nnQ6jfgEn/Np/5zHGtHre1FmmBumdAAAWkUlEQVScqar/UtVSVS1R1X/jDFlu/GBzXhkPvbee0wal\nc/e5Q/xXob1rKbx3B/Q4ASY+7J9zGGNChjfJol5ErhCRcLf57BVAfbN7mcNWV+/htldXERsZzkMX\nHe2/uSmqSuDVqyGpK1z2og0KaIxpljffEpcDlwC57utimpiXwvx40z/fyqpd+7lv0lD/DTleW+Uk\nipLdzsCAsS0wEKExptU7ZGsoEQkHLlDVSS0UT5u1YW8pjy/YxNlHd+LcYzr75yTVZfDWDbDlI5jw\ngE1cZIzx2iGfLNw5sS1R+Nn+ihquey6TxJgI7p801H/1FK9Pg6y34OTb4YQb/XMOY0xI8qYH95ci\n8k/gZeDbPheq+rXfompDVJV7385yJjL65fG0j4/y/Uk8HvjkIdjwrjPe00m3+v4cxpiQ5k2yOMH9\neV+jdQqc6vtw2p6/LdjIGyt2c/Np/TnWXwMELvwzfP6oMx2qNZE1xhwBb3pwj2+JQNqiuaty+MfH\nm7k0ozu/Ob2/f06y6EknUfQ+BS5+FvxVxGWMCWnNtoYSkSQR+ZuIZLqvR0UkqSWCC2W7Civ4/eur\nyeiZwv3n+6meIn8DLHwQeo6FyXMsURhjjpg3TWdnAqU4zWcvAUqAWf4MKtTVe5TbXlkFwGOXDicy\nwg/9HEr2wPMXQLsYuOC/EOnHyZKMMSHPmzqLvqrauMf2vSKy0l8BtQXTP9vK0u2FPHrxMLqn+ulL\n/P07oKIArn0fkrv75xzGmDbDmz9pK0XkxIYFERmLM2S5OQJrdhfztwUbOPvoTlx4bFffn6CmHD68\nF7LehJN/C11G+P4cxpg2x5sni+uBZxvVUxQBV/stohBWXl3Hb15eSUpsJA+cf7Tv6ylU4dVrYNN8\nOOZSOOFm3x7fGNNmedMaaiUwTEQS3eUSv0cVov787jq25pfx/NTRpMRF+vbglfthzuWw40s480E4\n/gbfHt8Y06Z50xrqQRFJdkecLRGRFBH5c0sEF0rW7C7m5WU7mXJCL8b28/H8FHXV8No1sGsJnPt3\nGPMr3x7fGNPmeVNncZaq7m9YUNUi4Gz/hRR6qmrrueXllXSIj+Lm03zcn2LfJvjPibDlYzjnMRg5\nxZrIGmN8zps6i3ARiVLVagARiQH8MCZF6PrTm2vYlFfGM9ccR3KsD4ufSvbA8xdC8S44/98w3AYD\nNsb4hzfJ4gXgIxGZhTPMx7XYXNxeW7SlgFeXZ3PD+L6MG5juuwNXlcCLF0NlIUxbaK2ejDF+5U0F\n919FZDVwGs60qver6ny/RxYCaus9PPz+ejomRvHrU31c/PTl45C7Bq58zRKFMcbvvHmyQFXfA97z\ncywh54F317Fq136euGw40e3CfXfg0r2wbAYMPhf6ne674xpjzEF40xrqQhHZJCLFIlIiIqUiYs1n\nm/HFpn0889V2rj6hF5OG+7DzXV01vHIV1NfC+N/77rjGGHMI3jxZ/BU4V1XX+TuYUFFcWcvvXltF\n37Q47jhrkG8PnjnLaSJ70dOQPti3xzbGmIPwJlnkWqI4PPfOXUteaTWvX3+Cb4uf6mqc4qcuI+Do\nn/ruuMYY0wxvkkWmiLwMvAlUN6xU1df9FlUr9unGfF5fsZubTu3HsO7Jvj34wj9DwSa45HnfHtcY\nY5rhTbJIBCqACY3WKWDJ4gDFlbX84Y3V9OkQxw2n9vPtwb98wnkNvQgGnePbYxtjTDO8aTp7zZEe\nXEQmAk8A4cAMVX24iW0uAe7BSUCrVPVyd309sNrdbKeqnnekcbQEVWeOir3FVbz8i+OJivBh8dPy\nZ2DBXTDkArjwKQjzw/wXxhhzCM0mCxHpBvwDGIvzhf4FcLOqZjezXzjwJHAGkA0sE5G5qprVaJv+\nwJ3AWFUtEpHGvdYqVXX44V5QoPzv6918uC6XP/5kMCN7+nAu7RUvwts3Q/oQuGA6hPkwCRljjJe8\n+RN1FjAX6AJ0Bd7Gu5nyRgGbVXWrqtYAc4BJB2xzHfCkO94UqprnbeDBpLC8hgfezWJkzxSuHdvb\ndwfe/gXMv9NJFFe+BhE+HqnWGGO85E2ySFPVWapa576eAdK82K8rsKvRcra7rrEBwAAR+VJEFrvF\nVg2i3Tm/F4vI+U2dQESmNcwNnp+f70VI/vHQvHWUVtXxwAVDCQvz0SB+JXvgufMBgctegMQuvjmu\nMcYcAW+SxT4RuVJEwt3XlUCBF/s19a2pByxHAP2BccBkYIaINDQh6qGqGcDlwOMi0vcHB1OdrqoZ\nqpqRluZN/vK9r3cW8erybKae1JtBnRJ9c1CPB969FdQDU+ZCah/fHNcYY46QN8niWuASYC+wB/ip\nu6452UDjyZ+7ATlNbPOWqtaq6jZgA07yQFVz3J9bgU+AoBsAqaSqlttf+4b0BB+P/fTurbBhHpz5\nAHQe5rvjGmPMEWo2WajqTlU9T1XTVDVdVc9X1R1eHHsZ0F9EeotIJHAZTt1HY28C4wFEpANOsdRW\nd4KlqEbrxwJZBJk/v5PF1vwyHrl4GPFRXg2z1bytn8DyWXDcz2H0L31zTGOM+ZG8GRvq2UZFQ7hf\n5DOb209V64AbgfnAOuAVVV0rIveJSEMz2PlAgYhkAQuB36lqATAYpzPgKnf9w41bUQWDhRvyeCUz\nm1+c0peTB/ioCKy+Fub9DpK6w4QHbBIjY0zQ8ObP4WMOnClPRLwqElLVecC8A9bd1ei9Are6r8bb\nfAUc7c05AqG4spY7/7ea/unx/OZ0HxY/LXoS9m2Ei5+FdtG+O64xxvxI3tRZhInItx0HRCQVL4c2\nD1V/fieL/LJqHrl4mO86323+ED682+mdPfhc3xzTGGN8xJsv/UeBr0TkNZzWTJcAD/g1qiC2cEMe\nry7P5lfj+vp27Kcvn4CEzvDTWdbxzhgTdLwZ7uM5EckETsVpDnthsNUftJSG4qcBHeO52ZfFT2vf\nhG2fOfUU1vHOGBOEvJ0pL4sgbI3UklSVu95aQ35ZNdOvGum74qfqMnjnFuh0DIya5ptjGmOMj7Xp\nuofD8drybN5amcNvJwzgmG4+LH5a/G+oLIQrbDgPY0zwsuFLvbA1v4y7565lTJ9Urh/nw6HHi7Ph\n04edSu1uI313XGOM8TFLFs3weJTbX/uGyIgwHr90BOG+GvvJ43FGkw2LgDMf9M0xjTHGTyxZNOOV\nzF1k7iji92cPplOSD/s+ZD7tNJc98wFI6em74xpjjB9YsjiE3JIqHpi3jtG9U7l4ZDffHbh4Nyx8\nEHqdBBlTfXdcY4zxE0sWB6Gq/PHNNdTUefjLRccgvhp6Q9UpfqqthIkP2ZAexphWwZLFQby7eg8L\nsnK5bcIAenWI892B182FzQvgtD9Bp6Ad0cQYY77HkkUTispruPuttQzrluTbme/qa+Gj+yBtsI0o\na4xpVayfRRPueyeLkqpa/vLT0USE+zCfrngeCjbD5Dk2pIcxplWxJ4sDLFyfxxsrdvOrcf18N/Md\nQE05fPIwdB8DAyY2v70xxgQRe7JopLSqlt+/4Yz99KvxP5jF9cdZ+CCU5cIlz1mltjGm1bEni0b+\n8v56ckuq+MtFx/hu7CeAvaudYT2OvQp6jPHdcY0xpoVYsnAt2VrAC4t3cu3Y3ozokdL8Dt6qq4E3\nfgmx7eH0e313XGOMaUFWDAXU1Hm4843V9EiN5bYJA3178K+egNw1cNlsiE317bGNMaaFWLIAnl+8\ng6355cy6+jhiIn1Y/FRXA0umQ/8JMOhs3x3XGGNaWJsvhtq2r5z/m7+ekwekMW5gmm8P/r+pUJ4H\no3/h2+MaY0wLa/NPFj1SY/n1qf25OKOb74b0ANjysdNbe/wfoN/pvjuuMcYEQJtPFuFhwg3jfThH\nBUBZnlOpndQDxt7s22MbY0wAtPlk4XP1tfDqNVBRCNfMg4ioQEdkjDE/miULX/vwHtjxBZz3D+g+\nKtDRGGOMT7T5Cm6f2vY5LPonjLzG6YBnjDEhwpKFr9RWwtwbIaW3TZNqjAk5VgzlK8uehqLtcNVb\nEBkb6GiMMcan7MnCFyoK4fNHoe+p0GdcoKMxxhifs2ThC+/cAlX74bS7Ah2JMcb4hSWLH2v315D1\nltOfosuIQEdjjDF+Ycnix1CFd2+FhE5w/K8DHY0xxviNVXD/GGtfh5wVMOlJiGsf6GiMMcZv7Mni\nSBXvhrk3QdogGDY50NEYY4xfWbI4EvV18NavoL4GLpoBYT4c1twYY4KQFUMdrroamHkm5HwNZ/0V\nOh0d6IiMMcbv/PpkISITRWSDiGwWkTsOss0lIpIlImtF5KVG66eIyCb3NcWfcR6Wpf91EsXZj8Co\naYGOxhhjWoTfnixEJBx4EjgDyAaWichcVc1qtE1/4E5grKoWiUi6uz4VuBvIABRY7u5b5K94vVJT\nDl885nS+G3VdQEMxxpiW5M8ni1HAZlXdqqo1wBxg0gHbXAc82ZAEVDXPXX8msEBVC93PFgAT/Rir\ndz79C1QUwLg7Ax2JMca0KH8mi67ArkbL2e66xgYAA0TkSxFZLCITD2PfllWyxxn/aehFNvS4MabN\n8WcFd1NzlGoT5+8PjAO6AZ+LyFAv90VEpgHTAHr06PFjYj00jwfevB7UA+N+77/zGGNMkPLnk0U2\n0L3Rcjcgp4lt3lLVWlXdBmzASR7e7IuqTlfVDFXNSEtL82nw3zPvNti6ECY+BB18PAWrMca0Av5M\nFsuA/iLSW0QigcuAuQds8yYwHkBEOuAUS20F5gMTRCRFRFKACe66lvf185A50+l4d2zwNMoyxpiW\n5LdiKFWtE5Ebcb7kw4GZqrpWRO4DMlV1Lt8lhSygHvidqhYAiMj9OAkH4D5VLfRXrAdVVw0LH4Ru\nxznTpEpTpWPGGBP6RPUHVQGtUkZGhmZmZvr2oF885sypfdVbNk+FMSYkichyVc1objvrwd0Ujwc+\nvAu++gcMmGiJwhjT5tnYUE1ZPstJFBlT4ZLnAx2NMcYEnD1ZHKg0Fz66D3qdBD951OopjDEGe7L4\nPlV4+yaoq4JzHrNEYYwxLnuyaCzrTdj4Pkx8GDr0D3Q0xhgTNOzJokF5AbxzCyT1sNFkjTHmAPZk\n0WDhA1BdCj97wyYzMsaYA9iTBcCeb+DrZ2Hk1dBlRKCjMcaYoGPJoiwf5lwOcelwSpPzMxljTJtn\nySIsHDoOhckvQbwfByM0xphWzOosYlPh8jmBjsIYY4KaPVkYY4xpliULY4wxzbJkYYwxplmWLIwx\nxjTLkoUxxphmWbIwxhjTLEsWxhhjmmXJwhhjTLNCZg5uEckHdvyIQ3QA9vkonNbCrjn0tbXrBbvm\nw9VTVZsdviJkksWPJSKZ3kxaHkrsmkNfW7tesGv2FyuGMsYY0yxLFsYYY5plyeI70wMdQADYNYe+\ntna9YNfsF1ZnYYwxpln2ZGGMMaZZbT5ZiMhEEdkgIptFJGSmyhOR7iKyUETWichaEbnZXZ8qIgtE\nZJP7M8VdLyLyd/f38I2IHBvYKzhyIhIuIitE5B13ubeILHGv+WURiXTXR7nLm93PewUy7iMlIski\n8pqIrHfv9/Ghfp9F5Bb33/UaEZktItGhdp9FZKaI5InImkbrDvu+isgUd/tNIjLlSONp08lCRMKB\nJ4GzgKOAySJyVGCj8pk64DZVHQyMAW5wr+0O4CNV7Q985C6D8zvo776mAf9u+ZB95mZgXaPlvwCP\nuddcBEx1108FilS1H/CYu11r9ATwvqoOAobhXHvI3mcR6QrcBGSo6lAgHLiM0LvPzwATD1h3WPdV\nRFKBu4HRwCjg7oYEc9hUtc2+gOOB+Y2W7wTuDHRcfrrWt4AzgA1AZ3ddZ2CD+/6/wORG23+7XWt6\nAd3c/0SnAu8AgtNZKeLAew7MB45330e420mgr+EwrzcR2HZg3KF8n4GuwC4g1b1v7wBnhuJ9BnoB\na470vgKTgf82Wv+97Q7n1aafLPjuH12DbHddSHEfu0cAS4COqroHwP2Z7m4WKr+Lx4HbAY+73B7Y\nr6p17nLj6/r2mt3Pi93tW5M+QD4wyy16myEicYTwfVbV3cAjwE5gD859W05o3+cGh3tffXa/23qy\nkCbWhVTzMBGJB/4H/EZVSw61aRPrWtXvQkTOAfJUdXnj1U1sql581lpEAMcC/1bVEUA53xVNNKXV\nX7NbjDIJ6A10AeJwimEOFEr3uTkHu0afXXtbTxbZQPdGy92AnADF4nMi0g4nUbyoqq+7q3NFpLP7\neWcgz10fCr+LscB5IrIdmINTFPU4kCwiEe42ja/r22t2P08CClsyYB/IBrJVdYm7/BpO8gjl+3w6\nsE1V81W1FngdOIHQvs8NDve++ux+t/VksQzo77aiiMSpJJsb4Jh8QkQEeBpYp6p/a/TRXKChRcQU\nnLqMhvVXua0qxgDFDY+7rYWq3qmq3VS1F869/FhVrwAWAj91Nzvwmht+Fz91t29Vf3Gq6l5gl4gM\ndFedBmQRwvcZp/hpjIjEuv/OG645ZO9zI4d7X+cDE0QkxX0im+CuO3yBrsAJ9As4G9gIbAH+EOh4\nfHhdJ+I8bn4DrHRfZ+OU1X4EbHJ/prrbC07LsC3AapyWJgG/jh9x/eOAd9z3fYClwGbgVSDKXR/t\nLm92P+8T6LiP8FqHA5nuvX4TSAn1+wzcC6wH1gDPA1Ghdp+B2Th1MrU4TwhTj+S+Ate6174ZuOZI\n47Ee3MYYY5rV1ouhjDHGeMGShTHGmGZZsjDGGNMsSxbGGGOaZcnCGGNMsyxZGOMD7sin34jILS18\n3k9EpE3NN20CI6L5TYwxhyIinYATVLVnoGMxxl/sycKENBHp5c7x8JQ7/8EHIhLjfjZcRBa7TwRv\nNDd0sztnwiwRWe0O2jfe/egDIF1EVorISQfskyYi/xORZe5rrLv+HhF5XkQ+ducZuM5dLyLyf+48\nDatF5NJGx7rdXbdKRB5udJqLRWSpiGxsOL+IDHHXrXSvr/+P/mWati3QvRTtZS9/vnCGeK4DhrvL\nrwBXuu+/AU5x398HPN7MsW4DZrnvB+EMOxHNAcNIH7DPS8CJ7vseOMOvANwDrAJigA44I4N2AS4C\nFuDM0dDRPUdnnIHyvgJi3f0beu5+Ajzqvj8b+NB9/w/gCvd9JBAT6Hthr9b9smIo0xZsU9WV7vvl\nQC8RSQKSVfVTd/2zOENCHMqJOF/CqOp6EdkBDAAONZrv6cBRzhBGACSKSIL7/i1VrQQqRWQhzuQ0\nJwKzVbUeZ9C4T4HjgFNwElWFe/7GA+E1DBK5HCdxASwC/iAi3YDXVXVTM9dmzCFZMZRpC6obva/n\nyOvqmhruuTlhOBPvDHdfXVW11P3swLF2DjakdMO5DzY2T8P1fXttqvoScB5QCcwXkVOPIHZjvmXJ\nwrRJqloMFDWqY/gZ8OkhdgH4DLgCQEQG4BQrbWhmnw+AGxsWRGR4o88mufUg7XEGPlzmnuNSceYR\nTwNOxhn87gPgWhGJdY+TeqiTikgfYKuq/h1nRNJjmonTmEOyYijTlk0B/uN+AW8FrgEQkV8CqOp/\nDtj+X+72q3HqQa5W1epGRUxNuQl4UkS+wfn/9hnwS/ezpcC7OEnnflXNEZE3cKYEXYXzJHG7OsOQ\nv+8mmkwRqQHmAb8/xHkvBa4UkVpgL06djDFHzEadNSYAROQeoExVHwl0LMZ4w4qhjDHGNMueLIwx\nxjTLniyMMcY0y5KFMcaYZlmyMMYY0yxLFsYYY5plycIYY0yzLFkYY4xp1v8HY4jq1kMtafkAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd81PX9wPHXOzsQEggbAoQ9FESJ\nbLe4xT2pFbWO1lWrtVq1Q9tfa111VQVnXVCtVpwIorhQCcreeychISRkX/L+/fH5hlxCIAfkcpfk\n/Xw87pH7rrv3Nwf3zmeLqmKMMcbsT0SoAzDGGBP+LFkYY4ypkyULY4wxdbJkYYwxpk6WLIwxxtTJ\nkoUxxpg6WbIwxhhTJ0sWxhhj6mTJwhhjTJ2iQh1AfWnXrp2mpqaGOgxjjGlU5s2bt0NV29d1XpNJ\nFqmpqaSnp4c6DGOMaVREZEMg51k1lDHGmDpZsjDGGFMnSxbGGGPqZMnCGGNMnSxZGGOMqZMlC2OM\nMXWyZGGMMaZOliyMMaYR+3TJdt78YWPQ38eShTHGNFKzlmdw3avzmDJ3E6oa1PdqMiO4jTGmqfOV\nV1BQWs7zX63lo0XbWJNVQN8OCUy9biQiEtT3tmRhjDFhxFdewcxlmWzeWUjW7hJWZ+xm085CWreI\nYVVGPjsLywAYkpLE9cf1YuLoVOKiI4MelyULY4wJkq25ReQUlPL+gq1szCmkY2IcI3u1JS21De0S\nYpnyw0be/GEja7IKOGlgB+KjI1m2PZ8Fm3L3vEZSfDQlvnJyC8vo2iaeC4elcPYRXRiS0rpB78WS\nhTHG1LOCEh8PTV/Bq99toLyielvCy9+uByA2KoISXwUdE2PpntyC9+ZvBSA6UrhoWAo3HN+bjolx\nJMSGx9d0eERhjDGN0KqMfP760TK25RbTs11LSssraBUXxQcLt1FeoZw7tAvDe7blqB6tSYqPZsvO\nIuZvyuUvHy7DV6FcMbIHt5zUl/atYtlVVEZeURmt4qJo3SIm1Le2Fwl2C3pDSUtLU5ui3BgTLKqK\nKkREuIbkjxdt45YpPxEVEUFyyxi27iqiVWwUecU+0nq04frjejNuUMd9vl5xWXmDtDXURUTmqWpa\nXedZycIYY/Zja24RV788l227iqlQZeLoVDonxfPHaYvpmBjHq9eMoGe7loBLKPklPhLjout83XBI\nFAfCkoUxxvjJ3l3CV6t2MKBzK6bO3cTb8zaTX+zj1MM6kltYxpOzVgMwoFMrXph4NF1bx++5VkQC\nShSNkSULY0yzlldcxmMzVvLG9xvp2jqetTsK9hyLjhROP7wzN57Qh/6dWgHw4cJtrM8u4NpjehET\n1XzGNQc1WYjIacDjQCTwvKr+vcbxicBDwBZv11Oq+rzf8URgGfCuqt4UzFiNMc1Hia+cWcsy+WDR\nNr5etYO84jJOGtCBLbnFDOvRhlG92uKrUK4ek0qHxLhq1545pHOIog6toCULEYkEngbGAZuBuSIy\nTVWX1jh16n4SwQPA7GDFaIwJX6pKRl4JMVGuAbnS7hIfqkpkhNAiZu+vMFVlxtIMvliZxcbsQp64\n7EiSW8agqnyyeDvPfrm22jiGxLgo3v3VGIZ2a9hxC41NMEsWw4HVqroWQESmAOcANZNFrURkGNAR\n+ASos6XeGNM0lFcoD36ynM+XZ7IqczcAt4/rR36Jj/T1Ofy4seqL/sJhKSTERnHPmQOJjozg2zU7\nuHzy99Ve75ynvyYuKpKy8grWZxcCbozDvWcNokdyC9JS29SadEx1wfwNdQU2+W1vBkbUct4FInIs\nsBK4TVU3iUgE8AhwBXDSvt5ARK4DrgPo3r17fcVtjGkAld32K+c02ryzkL99vJy1WQUs25bH6N5t\nSUtN5osVmTwyYyUAcdERXDmqB4u27OLHjbm8PW8zAKsy8+mSFM9b3vawHm2Y/PM03pu/hcdmrKQo\nqpzOSfFcNCyFv5x3OLFRjasnUjgIZrKobVarmoM63gfeVNUSEbkBeAU4EfgV8JGXOPb5Bqo6CZgE\nbpxFvURtjKlX5RXKd2uziY+JZFdhGXnFZXy0aBuLt+SRlV9Ct+R4BnZO5IOF2/Zc84uxPbn3rEF7\ntpdvz6OkrIKe7Vvu6W20bkcB7VvF8tcPl/LmD+7v0omjU/nNKf32nHPVmJ5cOSoVEYI+0V5TF8xk\nsRno5redAmz1P0FVs/02JwMPes9HAceIyK+ABCBGRHar6l1BjNcYU48KSnw8/9U65qzdwXdrc/Y6\nPrhrEh0SY1m6NY+MvBKO6duOEwd04LLh3fcagzCgU+Je11eObfjb+UOYOLonpb4KBqck7XVe5SA6\nc2iCmSzmAn1FpCeut9OlwOX+J4hIZ1Wt/HNiPK7nE6o6we+ciUCaJQpjwtMXKzLxlStj+rTjlTnr\n2VlQytJteSzYlEtesQ+AUw/ryHlHdiWv2EduYSknDexI7/YJgOuZFBURQeQhfKlXdms1wRO0ZKGq\nPhG5CZiO6zr7oqouEZH7gXRVnQbcIiLjAR+QA0wMVjzGmPo3b0MOE1+aW+uxgZ0TuWtkD84c3Jmk\nFvseqGbtB42DzQ1ljDkoS7bu4pqX08ktKuXG4/vw4aJtHNuvPecO7cq2XUWcOKCDtRM0AjY3lDGm\n3pRXKG/P28RHi7Zz9hFdSIiN5NYp81GFxy4ZyplDOnPzSX33nD+oy95tDKZxs2RhjNknVWVTThEP\nfLiUGUszAJi9MguAI1KSePaKYXROit/fS5gmwpKFMWaPHzfuZGtuEZ8tyyS3sJTPV2TtOXbvmQO5\nakxPnv9qLaszd3PPmQPDct0FExyWLIxpZioqlJ825VJSVs60BVtZkZFP25Yx5Bf7+H7d3l1cTx7Y\ngV8e34dhPdoAcP1xvRs6ZBMGLFkY00zkFpYyf1PuPnsvAYzq1ZbOSXH87vQBtEuIpcRXblNhGMCS\nhTHNwoJNuVz83BxKfBWAmw7jgqNSaN8qliO7t2b+xlxOGNBhr7EOlihMJfuXYEwTVlxWzpOzVjH5\ny3WUllfQLiGWZ392FGmpydXOO3k/y38aA5YsjGlyCkt9zFiaQYUqb36/iR/W53BEt9a8ctXR1iBt\nDpolC2OakOKycq5/dR5frdoBQFSEcNWYVH57an+rUjKHxP71GNOIqSrfrsnmzrcXsiW3aM/+QZ0T\nuev0AfTukFBtjWhjDpYlC2MaEV95BTOXZZDSpgU/bcrlpa/XVVszulVcFE9cdiQn9O8QwihNU2TJ\nwphGYvuuYm6Z8hM/+I2FGNCpFZekdeOitJS9Gq2NqU+WLIwJY3nFZcxekcV787cyc5mbbmPi6FRy\nCkpJbduCW0/ud0hTexsTKEsWxoSRjLxi1u0o4JvVO3j3py1syS2icmLovh0S+MPZgzimb/vQBmma\nJUsWxoTQkq27eHb2WtolxFDiq+CN7zdWOz66d1tuOK43vdq3JKVNixBFaYwlC2Ma3O4SH0/NWs0X\nKzJZvj2/2rFj+rajfatYzj8yhTF92tp6ECZsWLIwpgGVVygTJn/Hgs27aN0imiO7t+bmE/vQPbkF\nJb4KDuuy9xrSxoQDSxbGNJDVmbs5+dHZANw+rl+1xYKMCXeWLIwJstzCUl74eh3/nrMBgLtPH8B1\nx/YKcVTGHBhLFsYEyZqs3bw9bzPPfLEGcCvL/fW8wRze1aqaTONjycKYelJcVs7uEh/tEmL5ZPF2\nbnht3p5jd57WnxuO7U2EjYkwjZQlC2MOQXFZOaszd7N4yy4embGSrPySPcc6tIrlgXMP55i+7WwS\nP9Po2b9gYw7Sqox8Lp30HdkFpQC0bRlDcssYcgpKOXlgB+47axA92rYMcZTG1A9LFsb4KSz1sTJj\nN+/+uJle7RNIaRPPk7NWEx8dyaAuidw2rh+ZeW6OpsVb8kiIjeKG43qzq6iM358xgITYKEp8FcRF\nR4b6VoypV5YsgFnLMxjVqx3xMfYfvDnalFPI6szdLNm6i6c+X01xWUW143HRERSXVTBnbTYvfL2O\nmMgISsvdOc9dMYwxfdrVON/+HZmmp9kni43ZhVz9cjqHdUlkynUjaRUXHeqQTAMpK69g5tIMbn7z\nJ3wVumf/9cf14vLh3Vm+PZ/NO4s4d2gXYqIiuGzydwhCr/Yt+fXJ/ejZzqqYTPMhqlr3WY1AWlqa\npqenH/B1qsq9/1vM699vJCE2it+fMZDDuiQysHMiMVERQYjUhNq6HQV8tiyDV+asZ1NOEUnx0Zx3\nZFdOOawj/Tq2ol1CbKhDNKbBiMg8VU2r67xmX7IQEe47axBj+rTj8Zmr+P27iwBolxDDjNuOo01L\nW7O4KSivUD5dsp1ZyzN5a95mAOKjI/nHhUM4e0gXq4I0pg7NvmThr6DEx+yVWXy1Kos3f9hEt+R4\nPvvN8WTkFdOldbytG9DIqCorMvJ5K30zr87ZsKed4dh+7Tnj8E6cPKijlSJMs2cli4PQMjaKMwZ3\n5ozBnemcFM+jM1bS796PAejXMYHXrhlBh8Q4AEp9FVSokplXQnxMJO1b2ZdOOMgpKOXvHy9j6bY8\nFm/J27O/dYtorh3Rk0uP7k63ZJvq25gDZcliH351fG+Wbs1j7vocsgtKWZmxm9vfWsDDFx3Blyuz\nuP+DpZT6KlCFiAj48b5xNvAqxFZsz+f2t+bvSRJDUpI4oX8HLhvenU5JcSGOzpjGzaqhAnTv/xbx\n2ncb93vOr0/uy+rM3QBcnNaNY/vZimYNobxCeeCDpbz87XoA/jz+MEb0SmZAp8TQBmZMI2DVUPXs\n9nH9ySkoZVXGbsYf0YUzhnSme3ILIkX42Qvf8+2abP45c9We8z9YuI0/nj2InzbmMm3BViaM6M5f\nzj3cFrOpJxl5xdzx1gKKSsvZmFNIZn4Jx/Rtx2/G9ePI7m1CHZ4xTY6VLOqBqlJUVs6s5Zn0bNeS\npPhobnhtXrU6c4DoSOGitG785ZzDbUK5AFX++/RPssVl5Vz47Ld7fr9pPdpwxagenDO0a0hiNKYx\nC7RkUWeyEJEOqppZY19/VV1xiDHWq1Ami9rkFpby9rzNtE2IYWyf9jw6YyUZecXMWu5+lW9eO5JR\nvduGOMrwlVNQyjerd/DMF2vIzC/hvrMGsjW3mH/PWc+2XcUAXDO2Jzef2IfWLax7szEHqz6TxQrg\nPlX9j7d9O3CNqg6ql0jrSbgli9qoKpdP/p45a7MB+OXxvfntKf2DVspQVdZkFRAVISzfns/UuRsZ\nP7QL44/oWq0bcGGpjzWZBQxOCf06CwUlPl6Zs55/zlhFaXkFbVpEkxQfzfrswmrnXX9sL+4+Y2Bo\ngjSmCanPZNEZmAQUAx2BZcDtqrq7PgKtL40hWYD7Ap+9Movf/XchGXkldEuO5+SBHTlrSBc6Jsby\n+YosTh3UsVpX3EDaOXYVlvH16h1Unvqf9E2owuyVWXud2yUpjstHdOfV7zZQVq7ER0eyJbcIgPvO\nGsTE0akNPqZk5tIMfvffhXtmcO2UGMcD5x7O8J7JtIiJ5OPF21m0OZdTD+tEWmpyg8ZmTFNWb8nC\ne7EbgbuBCuAyVf3m0EOsX40lWVRSVZ74bDX/+mI1Jb7qE9dFRwpl5VWfy4BOreicFMd1x/YmPiaS\nldvzGT+0C9kFpfx33mbmrs/hq1U79nqPmKgIbjy+D3HREZT6Khjdpx1//XApP27MBaBr63iyC0qI\njoggv8S357rhqclMGNmdET3b8snibZx3VApJ8cGZM2vm0gzueHsBuYVlAAzr0YZrj+nJCQM6EBtl\no6qNCbb6LFnMALYBtwApwIvAl6p6R30EWl8aW7KoVFGhfLNmBw9NX8Go3m1Zv6OAr1btoLC0nOGp\nycRGR5CVX8Ly7fl7XRsT5ZJApb+fP5h12QV0Tozj4qO7ERMZQVRk9fmtisvK+XHDTpJaRNOnQwKF\nJeW0jI0iJiqC4rJy/jRtCVPmbtrrvaZcN5IjUlrvKXEEMm9W5cJAfTsmsGJ7PvHRkYgIny/PJCOv\nmGkLtpLpLRZ0xcge3HX6AFrGWgc9YxpSfSaLc1X1f37bUcDdqvpAAEGcBjwORALPq+rfaxyfCDwE\nbPF2PaWqz4tID+Ad77po4ElVfXZ/79VYk0WgMr3G8ejICBZuzmVV5m7KyisY1astFx/dja6t4+ut\nW+7UuRtZti2fzPxitu8q3lMSAWgVG8Wo3m157ophtb7f1twivlubzTers5m2YEu1ElJtjuremtd+\nMcIGNBoTIvVdDdUD6KuqM0UkHohS1b3/1K1+TSSwEhgHbAbm4qqwlvqdMxFIU9Wbalwb48VWIiIJ\nwGJgtKpu3df7NfVkEUoZecVc8cL3rMyoaqY6oX97WsREkdImnjtO7U90ZAQrM/K5+Lk5e6qUDu+a\nyNlDuvD+wq1k7y6lTYsYSssruGhYCqce1onICLGpN4wJsXoblCci1wLXAclAb1xV1LPASXVcOhxY\nraprvdeZApwDLN3vVYCqlvptxgI2V3gIdUyM45Nbj2Xu+hwGpyRxz7uLefenLXuOP/fl2j3Pk+Kj\n+ceFQ4iLjuS0wzoRExXB9cf1DkXYxph6FEjZ/0bcF//3AKq6SkQ6BHBdV8C/8nszMKKW8y4QkWNx\npZDbVHUTgIh0Az4E+gC/ra1UISLX4RIZ3bt3DyAkc7AiIoQRvdy4kMcuGcpFw1Ioq1A25RQy6cu1\nbMwpZGDnRJ68bCh9OrQKcbTGmPoWSLIoUdXSyvppr80ikGHftVWg17zufeBNr7rpBuAV4EQAL2kM\nEZEuwP9E5G1Vzaj2YqqTcN16SUtLaxpD0RuJ0X5Lif5sZI8QRmKMaQiBVO/MFpHfA/EiMg54C/cl\nX5fNQDe/7RSgWulAVbNVtcTbnAwMq/kiXoliCXBMAO9pjDEmCAJJFncBWcAi4HrgI+DeAK6bC/QV\nkZ5eg/WlwDT/E7wBf5XG4wb8ISIpXkM6ItIGGAOE1fQixhjTnNRZDaWqFbi/+icfyAurqk9EbgKm\n47rAvqiqS0TkfiBdVacBt4jIeMAH5AATvcsHAo+IiOKqsx5W1UUH8v7GGGPqzz67zorIIvbTNqGq\nQ4IV1MGwrrPGGHPg6qPr7Fnezxu9n696PycAhXufbowxpqnaZ7JQ1Q0AIjJGVcf4HbpLRL4B7g92\ncMYYY8JDIA3cLUVkbOWGiIwGWgYvJGOMMeEmkHEW1wAvikjlYge5wNXBC8kYY0y4CaQ31DzgCBFJ\nxDWI7wp+WMYYY8JJIHNDxQIXAKlAVOVIblW1NgtjjGkmAqmGeg/YBcwDSuo41xhjTBMUSLJIUdXT\ngh6JMcaYsBVIb6hvRWRw0CMxxhgTtgIpWYwFJorIOlw1lAAabiO4jTHGBE8gyeL0oEdhjDEmrO0z\nWYhIoqrmAftdPtUYY0zTt7+SxRu4+aHm4SYU9F/MSIFeQYzLGGNMGNnf3FBneT97Nlw4xhhjwlEg\nvaGMMcY0c5YsjDHG1MmShTHGmDoFlCxEZKyIXOU9by8i1o5hjDHNSJ3JQkT+CPwOuNvbFQ28Fsyg\njDHGhJdAShbnAeOBAgBV3Qq0CmZQxhhjwksgyaJUVRU3tgIRsVXyjDGmmQkkWfxHRJ4DWovItcBM\nYHJwwzLGGBNOAlkp72ERGQfkAf2BP6jqjKBHZowxJmwEslJeT+CrygQhIvEikqqq64MdnDHGmPAQ\nSDXUW0CF33a5t88YY0wzEUiyiFLV0soN73lM8EIyxhgTbgJJFlkiMr5yQ0TOAXYELyRjjDHhJpDF\nj24AXheRp3DTlG8Cfh7UqIwxxoSVQHpDrQFGikgCIKpqiyEZY0wzE0hvqFjgAiAViBJxayCp6v1B\njcwYY0zYCKQa6j1gF27FvJLghmOMMSYcBZIsUlT1tKBHYowxJmwF0hvqWxEZHPRIjDHGhK1AShZj\ngYkisg5XDSWAquqQoEZmjDEmbASSLE4PehTGGGPCWiBdZzcAiEgHIC7oERljjAk7gayUN15EVgHr\ngNnAeuDjIMdljDEmjATSwP0AMBJYqao9gZOAb4IalTHGmLASSLIoU9VsIEJEIlT1c2BoIC8uIqeJ\nyAoRWS0id9VyfKKIZInIfO/xC2//UBGZIyJLRGShiFxyQHdljDGmXgXSwJ3rTfXxJW6OqEzAV9dF\nIhIJPA2MAzYDc0VkmqourXHqVFW9qca+QuDnqrpKRLoA80RkuqrmBhCvMcaYehZIyeIcoAi4DfgE\nWAOcHcB1w4HVqrrWm9Z8ivdadVLVlaq6ynu+FcgE2gdyrTHGmPoXSG+oAr/NVw7gtbviZqittBkY\nUct5F4jIscBK4DZV9b8GERmOWz9jzQG8tzHGmHq0z5KFiOSLSN6+HgG8ttSyT2tsvw+kegP8ZlIj\nGYlIZ+BV4CpVrahxLSJynYiki0h6VlZWACEZY4w5GPssWahqKwARuR/YjvvSFmAC0CqA194MdPPb\nTgG21niPbL/NycCDlRsikgh8CNyrqt/tI8ZJwCSAtLS0monIGGNMPQmkzeJUVf2Xquarap6qPoOb\nsrwuc4G+ItJTRGKAS4Fp/id4JYdK44Fl3v4Y4F3g36pq630bY0yIBZIsykVkgohEikiEiEwAyuu6\nSFV9wE3AdFwS+I+qLhGR+/2Wab3F6x67ALgFmOjtvxg4FjcnVWW32oC66xpjjKl/orr/2hsRSQUe\nB8bg2hy+AX6tquuDHNsBSUtL0/T09FCHYYwxjYqIzFPVtLrO229vKG+sxHmqGlCXV2OMMU3Tfquh\nVLWcAMdGGGOMaboCGcH9jYg8BUwF9oy5UNUfgxaVMcaYsBJIshjt/bzfb58CJ9Z/OMYYY8JRICO4\nT2iIQIwxxoSvQNazSBKRRytHSovIIyKS1BDBGWOMCQ+BjLN4EcjHjX24GMgDXgpmUMYYY8JLIG0W\nvVXVf8T2n0VkfrACMsYYE34CKVkUicjYyg0RGYObstwYY0wzEUjJ4pfAK37tFDupmpbDGGNMMxBI\nb6j5wBHeLLCoaiDTkxtjjGlCAukN9X8i0tqbcTZPRNqIyF8aIjhjjDHhIZA2i9P9175W1Z3AGcEL\nyRhjTLgJJFlEikhs5YaIxAOx+znfGGNMExNIA/drwGci8hJumo+rObC1uI0xxjRygTRw/0NEFgEn\n4ZZVfUBVpwc9MmOMMWEjkJIFqvox8HGQYzHGGBOmAukNdb6IrBKRXSKSJyL5ImLdZ40xphkJpIH7\nH8B4VU1S1URVbaWqicEOrMFUVMCXD0HeVljxMfhKQx2RMcaEnUCqoTJUdVnQIwmVnLXw1aMwyxs6\n0m0EjLsfuo8MbVzGGBNGAilZpIvIVBG5zKuSOl9Ezg96ZA2lXR+49PWq7U3fw4unQs660MVkjDFh\nJpCSRSJQCJzit0+Bd4ISUSj0PhFG3ACdBkNca5g6AT67Hy6ymdiNMQYC6zp7VUMEEnKnP1j1fMyt\n8M3jkHI0jPpV6GIyxpgwEUhvqBQReVdEMkUkQ0T+KyIpDRFcyIy62f2cfje8fjEU5oQ2HmOMCbFA\n2ixeAqYBXYCuwPs09ZXyEtrDFe+656umw7vXQ/aa0MZkjDEhFEiyaK+qL6mqz3u8DLQPclyh1/tE\nuHo69DwOVn0KTx4Ff0qCBVNDHZkxxjS4QJLFDhH5mYhEeo+fAdnBDiwsdB8Jl0+FE+6p2vfudW5s\nxpx/waf3hS42Y4xpQKKq+z9BpDvwFDAK1wvqW+BWVd0Q/PACl5aWpunp6cF7A1WYdhP89Fr1/e0H\nwJmPQpcjIaZF8N7fGGOCQETmqWpanefVlSwai6AnC3BjL54YWvuxuNZw5zqICKSwZowx4SHQZBFI\nb6hXRKS133YbEXnxUANslJJ7wg3fwGVT4U+74LrZ0NlLHsW58NOroY3PGGOCJJA/g4fUslLekcEL\nKcx1Ohz6n+aedxkK18+GuzZByw7w2Z+htCC08RljTBAEkiwiRKRN5YaIJBPg1ObNRlwijPszFGZD\nxpJQR2OMMfUukGTxCPCtiDwgIvfjGrj/EdywGqHUY9zPmX+CsmL3vKK86mdhDqz/OiShGWPMoQpk\nuo9/i0g6cCJupbzzVXVp0CNrbFp3g9E3w7dPwtePQmJXeP8WiG8DRTurzrspHRI6utKIMcY0EtYb\nqj5VlMM/ekLxrv2fFxEFv5jputuWFkJ0PIgE9h4//hu+fBgueB46DITYVocetzGm2aq33lDmAERE\nwsl/gugW0KIdjH8S/pgLv1nmek+Nu9+dV+GDSce7EeH/1xk+vD2w1y/OcwMBczfAC+Pgbymw4dt9\nnz9lAnzy+0O8KWOMsZJFw/OVwrdPwKwHqu+/bakrXfzvV3DsHdCmJ8x/HX6YDOc9C92Gw6vnwea5\nLgnNfR62LXDX3rEaWrZza3F0HQY718Oit2H2393x/mfAcXe6kowxxvixQXnhrKICinKqJif893hI\n6gbZq+q+duB4uMQbz7HobfjvNRCfDAkdIGv5/q+d+CGkjj202I0xTYpVQ4WziAhXEug+wj3G3Fp7\nojjxPrjyg6rtvqfARa9UbQ++EHqd4BKPf6KIbgFjfg33bIcT7nWlFIAZfwjO/RhjmryglixE5DTg\ncSASeF5V/17j+ETgIWCLt+spVX3eO/YJMBL4WlXPquu9GlXJoqZyHyz9H/Q81vWeqiiHgizXwwpg\n4/eu5JDcc+9rd22BZdOgaxp0O3rf7/H1P2HmH+GMh2H4tcG5D2NMoxPyaigRiQRWAuOAzcBc4DL/\nbrdeskhT1Ztquf4koAVwfZNPFg0hfzs80t9Vd934g016aIwBwqMaajiwWlXXqmopMAU4J9CLVfUz\nID9YwTU7rTrB6Q/Brk3wxJFuUkRVN4vu9kWhjs4YE+aCOW1HV2CT3/ZmYEQt510gIsfiSiG3qeqm\nWs4x9eHw8+GLv8Hu7W723Kg48BW7cR+3r3DtKMYYU4tglixqG2VWs87rfSBVVYcAM4FX9r5kP28g\ncp2IpItIelZW1kGG2Yy0bAe/Wwe/mAVjb3ON40MnuHEf026G3I2hjtAYE6aCWbLYDHTz204Btvqf\noKr+K+5NBh48kDdQ1UnAJHBtFgcXZjOUMsw9KmWvhhUfucfp/4AR14cuNmNMWApmyWIu0FdEeopI\nDHApMM3/BBHp7Lc5HlgWxHgH/g//AAATPUlEQVTMvlz5ASSmuOcf3wk7VsE3j8Mr42G3ldiMMUEs\nWaiqT0RuAqbjus6+qKpLvJlr01V1GnCLiIwHfEAOMLHyehH5ChgAJIjIZuAaVZ0erHibtagY+M0S\n2DIPJp8IT/l1jJh0PJz/nA3mM6aZsxHcprovH3ZTivQ+0c2cO3WC23/HKjfWI3eTm8xw4xw3e+4Z\nD0GL5NDGbIw5aIF2nbVFjEx1x95RffvMR+HD38DrF8G2+Xufn7EYfjnH1h43pomz/+Fm/46+Bk75\nq1u8qdJpf4frvoB+p7lpRlZZ7aAxTZ2VLEzdRt8Eo26EVTOgVUfofITbf8pfYeUnbrbb/qeHNkZj\nTFBZycIERgT6nVKVKADa9YEOh8HXj8Gbl0HWCjcq3BjT5FiyMIfm8qnQfZQbo/H0cHjnWjcRoqol\nDmOaEEsW5tC07gY/fw+OuMxtL3oL7k+GP7d2izVVlIc2PmNMvbBkYQ5dVKxbze/Xi6vvX/s5fPy7\n0MRkjKlX1sBt6k/rbm6t8a3z3RiNl8+AuZOhx2g3iaExptGykoWpf12GQkJ7uOQ1t/32VfD536qO\nF+10S8v6SqHEm4V+dxbM+gsU5cKS/8E/B8OO1Q0fuzGmVjaC2wRXzjo3HTq4Ed+7M/Y+p99prgsu\nuPXEi7wxHUdf60aIS20TGBtj6kM4LH5kjFsK9upPIa517YkCXKKIinfPi3Kg20ho1dlVYT2VBlkr\nGy5eY0ytrM3CBF/3EXDXhur7yopgxcduCpHSAjjtQdAKyN8KrXu4XlSf/xXSX4CXToNL34DuI2t/\n/YJsqChzqwEaY4LCqqFMeMteAy+fBeUlcO4z0HmoG0Veafo9MOepqu2zn4BhVzZ8nMY0UlYNZZqG\ntr3hwhehMBveuBge6QfFu+DLh+D5cdUTBcD7t0DO2tDEakwTZsnChL8eo+CqT6q2/zXa9Zza/IPb\nvvpT+GOuWyYW4IkjbfS4MfXMkoVpHHqMcmM4WveAvM1uipG+p7hE0X2E6zF1/qSq87/5Z+hiNaYJ\nsgZu07hcNsX1nhp0jqui8pfQAe7LhhdPgZl/goHj9z7HmKZEFSp8UF4KMS2D+lZWsjCNS8dBcMxv\n9p0EIqNcIzfAU0dbdZRpurLXwL9GwQPt4K2JQX87Sxam6ekwyP3Ucnj2GNi+eP/nG9OYZCx13c2/\nfBiylkFiCvQ5Oeh/GFk1lGl6IiLg2lkw+UTIWAQvjIPT/gbDJoY6MmMOjCos/q9bYGztF9C+Pyx7\nv+r44IvggucbJBRLFqZp6joMbvwBMpfB5/8H79/q5qEafXOoIzMmcD9Mgo/vrNre4c1mENMKRlwP\nI3/ZYKHYoDzT9JX74O2JVX+RJaa4UsbYX0NkdCgjMw2p3AerPnUzAbRIDnU0e6uocL36KudCKy2A\nZ0a7iTdPvM/1ACzIBAR6HV9vc6YFOijPkoVpHnwl8MYlbo2NSrGJMOQSOO5O15PKhAdfCcx/3X0h\nJvfaz3mlLtlnrQBfsZvtuKaiXIhLcn+Rv3Yh7NoIbVLh+Lth0LkQHRekmzhAvlJX+t36o5tYc/mH\nkL3KHTv6F3DmI0F7a0sWxtSkCgU73HxTy953f7HlbYHYJLh9OcS0CHWEjZOvBEp2Q8u21ffvWOW+\nzNukBvg6peArgpl/dp8RwKVvwoAz3POyIte42yIZ4tvAi6e5Bt5Kf9gJBVnQoi2smw2f3e/mHvMX\nEe3mEavUcbBr1wK4+cf66WpdUQEf3QHrv3bdWRM6uvc89W/Qvp/7d7hrE2z6wS1HnNgVvvuX6wK7\nJ84o11EjJQ3GPQCxCYce1z5YsjAmEF894r5Uzn4c8rdDVByMudX1Wy8vhdhWoY4w/L1+MayeAf3P\ngOgWLgnv2lz1RX7WYzDsKsjdAGs+d+1Iw69zX/iVCaZkNzwzCnI3Vn/tlh3coMu2fV1iXzi1+vE2\nPWHnOvc8qZv7Eq4UnwxJKbB9oSuhjLgBjrzCvf+Kj+Cn11yJcvvCqmsufOnAF+pSdUlq4xw3GeaK\nj/eOszK+I6+ALemuOqymFu3g8v9AWaGbrTkp5cDiOEiWLIwJRFEuPNij+r6xv4Gs5e4L5b5sN3bD\n1G7rfJh0XO3HIqKq/7Vcm8EXQ95W2PC1227ZHvqfDsf/HlbPhBl/qFrfpPI1R9wAi9+Bw851vdzm\n/Aum3+1d3wEGng3Zq+GUv0DnIbDuK+h42N7tFKqu3l8VJh1fVQqp/MxXTnftBm37uNep6fvnYO1s\nWPHh3sdiE+GKd13vpfIyV5L9+E5XXSaR0G0EpI6Fwy9wJa+IyJC1n1myMCZQf052YzIufQOmXF79\n2KVvwIAzQxNXY/DWRFj9Gfx6oZvgcfVM92VYVuR66qybDf8+x52bdjX0P9OtorjiY9cu4V+SGPoz\nOPfp6q9fUQFL3nHroexYAUdctveXvq8E5r0MXY6Cbkcf/L1M/Vn1bqn+fj4N5jwNG79zpYfS/OrH\nj7gcjrjE9bhLPQbiW+/9GgXZLjnFtwmrBb0sWRgTKF8JIBAV4xZaeucX7i/D9V/B0Alw6l9h/huQ\ncrT7Ilz8X0gZBj3GuDU0stdAYheIjg/1nTSskt3wYCoMv9b9hX/A1+fD14+5Bt1uw+s9vANWXgbv\n3QQLp7jt1j0gJgEyl7jtiGjoeQysmeW2D7/QJcCcNS7RRTTOMc6WLIw5VC+eDhu/3f850S28OuZe\ncNM894WRsxYWTHHVWcHqbVNR4b7U2vY5uC/anetd7HX1Alv8jkuO3Ya7dgb/hLjuK3jlLJjwNvQd\nd+AxhKOyYlfaad0domJdCWDtF64x+vALXAN4cZ6rTmoiPegsWRhzqLJWwoe/cSWMmk7/h6v68BVX\nrZ/RZ5zr5vjmJW47qTtMfD/w3kAH4sM73LKzkTHw2zWuMb5lu8CuLch266KX5Ll1zk/9q/tiBDcW\nYUs6pAyH/G3w2KCq6wZf7L4gty2AwRfChjkuYd22FJK61v89mgZhycKY+lC8y3Xl7HS4q6Y4/AL3\nM7Fz1SCq3RluShH/+vfBF8GS/7lE8as5gTVeqsLc5129//inXN14XKL7az53k6szX/NZ1Sjetn1c\nQ26l8U/CUT+vmiOotnrx/O3w9Agozq3aF5sIN6W75PHS6a5nT6subsGp8hK44AX44u9V/f79JXV3\n7RVhVAdvDowlC2MaUslu+Jv31/X5z8OQi2DpNPjPFa47Ztdhruvk/uq1F/4H3rm2+r74ZPjlNzD5\nJLc+ub8rP3DVQJUiY9z5u70uwH1OhrG3uWqktbNdG8EuL6H1Ox36nASf3LXvHksJneDyKdDlSFd6\n+vxvruE25eiqJDXsKpc4TaNlycKYhvbjq/DjK/Dz99xgLF8pPHaYN0UD++7DX+6DJe/CrAfcWITj\nfw9f/N/e5539uGtwn/+GK3UMm+h65yR0cO0Pn94Li9/ed3wS4RLW6Jth6OUuxuI8eCrNlY4AzpsE\ng8a7AXVJKeE5LYapV5YsjAkHS96tvtZA2z7Q8zgYeBb0GOtG9j6ZVlVqOPcZ90Wu6v6az1zmunR2\nPByu/ayqbaEuhTkw/R43vUmXI10bS0xL96jtNfK2uu6pNoq92bFkYUy4WPYBTJ2w9/7kXm7Fv68f\nc9sn3AvH/Xbv83ylrluvMUFgycKYcFJR4UoPLdq5nwv/A194YxNSj4Er37dGYhMSgSYLm8fAmIYQ\nEVE1109yLzj+LtdDaucGOOEeSxQm7FmyMCZUjrk91BEYE7DGOT7dGGNMgwpqshCR00RkhYisFpG7\najk+UUSyRGS+9/iF37ErRWSV97gymHEaY4zZv6BVQ4lIJPA0MA7YDMwVkWmqurTGqVNV9aYa1yYD\nfwTSAAXmedfuDFa8xhhj9i2YJYvhwGpVXauqpcAU4JwArz0VmKGqOV6CmAGcFqQ4jTHG1CGYyaIr\n4LdsFZu9fTVdICILReRtEel2gNcaY4xpAMFMFrX1Baw5qON9IFVVhwAzgVcO4FpE5DoRSReR9Kys\nrEMK1hhjzL4FM1lsBrr5bacA1WZCU9VsVS3xNicDwwK91rt+kqqmqWpa+/bt6y1wY4wx1QUzWcwF\n+opITxGJAS4FpvmfICL+01WOB7wV3pkOnCIibUSkDXCKt88YY0wIBK03lKr6ROQm3Jd8JPCiqi4R\nkfuBdFWdBtwiIuMBH5ADTPSuzRGRB3AJB+B+Vc3Z6038zJs3b4eIbDiEkNsBOw7h+sbI7rnpa273\nC3bPB6pHICc1mbmhDpWIpAcyP0pTYvfc9DW3+wW752CxEdzGGGPqZMnCGGNMnSxZVJkU6gBCwO65\n6Wtu9wt2z0FhbRbGGGPqZCULY4wxdWr2yaKumXEbKxHpJiKfi8gyEVkiIrd6+5NFZIY3m+8MbxwL\n4jzh/R4WishRob2DgycikSLyk4h84G33FJHvvXue6o37QURive3V3vHUUMZ9sESktTddznLv8x7V\n1D9nEbnN+3e9WETeFJG4pvY5i8iLIpIpIov99h3w51pfM3g362ThNzPu6cAg4DIRGRTaqOqND7hd\nVQcCI4EbvXu7C/hMVfsCn3nb4H4Hfb3HdcAzDR9yvbmVqgGeAA8Cj3n3vBO4xtt/DbBTVfsAj3nn\nNUaPA5+o6gDgCNy9N9nPWUS6ArcAaap6OG4c16U0vc/5ZfaeQPWAPle/GbxH4CZ3/WNlgjlgqtps\nH8AoYLrf9t3A3aGOK0j3+h5uuvgVQGdvX2dghff8OeAyv/P3nNeYHripYT4DTgQ+wM0ztgOIqvmZ\n4waMjvKeR3nnSajv4QDvNxFYVzPupvw5UzXRaLL3uX2Am6m6yX3OQCqw+GA/V+Ay4Dm//dXOO5BH\nsy5Z0Exmt/WK3UcC3wMdVXUbgPezg3daU/ld/BO4E6jwttsCuarq87b972vPPXvHd3nnNya9gCzg\nJa/q7XkRaUkT/pxVdQvwMLAR2Ib73ObRtD/nSgf6udbb593ck0VAs9s2ZiKSAPwX+LWq5u3v1Fr2\nNarfhYicBWSq6jz/3bWcqgEcayyigKOAZ1T1SKCAqqqJ2jT6e/aqUc4BegJdgJa4apiamtLnXJd9\n3WO93XtzTxYBzW7bWIlINC5RvK6q73i7MyoncPR+Znr7m8LvYgwwXkTW4xbbOhFX0mgtIpXzoPnf\n15579o4n4eYoa0w2A5tV9Xtv+21c8mjKn/PJwDpVzVLVMuAdYDRN+3OudKCfa7193s09WdQ5M25j\nJSICvAAsU9VH/Q5NAyp7RFyJa8uo3P9zr1fFSGBXZXG3sVDVu1U1RVVTcZ/lLFWdAHwOXOidVvOe\nK38XF3rnN6q/OFV1O7BJRPp7u04CltKEP2dc9dNIEWnh/TuvvOcm+zn7OdDPtf5m8A51A06oH8AZ\nwEpgDXBPqOOpx/saiytuLgTme48zcHW1nwGrvJ/J3vmC6xm2BliE62kS8vs4hPs/HvjAe94L+AFY\nDbwFxHr747zt1d7xXqGO+yDvdSiQ7n3W/wPaNPXPGfgzsBxYDLwKxDa1zxl4E9cmU4YrIVxzMJ8r\ncLV376uBqw42HhvBbYwxpk7NvRrKGGNMACxZGGOMqZMlC2OMMXWyZGGMMaZOliyMMcbUyZKFMfXA\nm/l0oYjc1sDv+4WINKv1pk1oRNV9ijFmf0SkEzBaVXuEOhZjgsVKFqZJE5FUb42Hyd76B5+KSLx3\nbKiIfOeVCN6ta+pmb82El0RkkTdp3wneoU+BDiIyX0SOqXFNexH5r4jM9R5jvP1/EpFXRWSWt87A\ntd5+EZGHvHUaFonIJX6vdae3b4GI/N3vbS4SkR9EZGXl+4vIYd6++d799T3kX6Zp3kI9StEe9gjm\nAzfFsw8Y6m3/B/iZ93whcJz3/H7gn3W81u3AS97zAbhpJ+KoMY10jWveAMZ6z7vjpl8B+BOwAIgH\n2uFmBu0CXADMwK3R0NF7j864ifK+BVp411eO3P0CeMR7fgYw03v+JDDBex4DxIf6s7BH435YNZRp\nDtap6nzv+TwgVUSSgNaqOtvb/wpuSoj9GYv7EkZVl4vIBqAfsL/ZfE8GBrkpjABIFJFW3vP3VLUI\nKBKRz3GL04wF3lTVctykcbOBo4HjcImq0Ht//4nwKieJnIdLXABzgHtEJAV4R1VX1XFvxuyXVUOZ\n5qDE73k5B99WV9t0z3WJwC28M9R7dFXVfO9Yzbl29jWldOV772tunsr723NvqvoGMB4oAqaLyIkH\nEbsxe1iyMM2Squ4Cdvq1MVwBzN7PJQBfAhMARKQfrlppRR3XfArcVLkhIkP9jp3jtYO0xU18ONd7\nj0vErSPeHjgWN/ndp8DVItLCe53k/b2piPQC1qrqE7gZSYfUEacx+2XVUKY5uxJ41vsCXgtcBSAi\nNwCo6rM1zv+Xd/4iXDvIRFUt8atiqs0twNMishD3/+1L4Abv2A/Ah7ik84CqbhWRd3FLgi7AlSTu\nVDcN+SdeokkXkVLgI+D3+3nfS4CfiUgZsB3XJmPMQbNZZ40JARH5E7BbVR8OdSzGBMKqoYwxxtTJ\nShbGGGPqZCULY4wxdbJkYYwxpk6WLIwxxtTJkoUxxpg6WbIwxhhTJ0sWxhhj6vT/EFLbPgyoOaAA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the final metrics\n",
    "# print('Train C-Index:', metrics['c-index'])\n",
    "# print('Valid C-Index: ',metrics['valid_c-index'][-1])\n",
    "\n",
    "print(\"num of epochs: \", len(metrics['train_loss']))\n",
    "# print(metrics['train_loss'])\n",
    "# Plot the training / validation curves\n",
    "plt.plot(range(len(metrics['train_loss'])), metrics['train_loss'])\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('training loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(metrics['c-index-soft'])), metrics['c-index-soft'])\n",
    "plt.plot(range(len(metrics['c-index-valid-soft'])), metrics['c-index-valid-soft'])\n",
    "\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('concordance index')\n",
    "# plt.plot(range(len(metrics['c-index'])), metrics['c-index'])\n",
    "plt.xscale('linear')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(len(metrics['c-index-hard'])), metrics['c-index-hard'])\n",
    "plt.plot(range(len(metrics['c-index-valid-hard'])), metrics['c-index-valid-hard'])\n",
    "\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('concordance index')\n",
    "# plt.plot(range(len(metrics['c-index'])), metrics['c-index'])\n",
    "plt.xscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>lenfol</th>\n",
       "      <th>fstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.44662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.589239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.64232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.48160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1453.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.58821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.12942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.616980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.67519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.46412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.423138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.26457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1567.236000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.46070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1349.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.41255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278.901600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.48131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1965.911000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.27114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1929.816000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.22902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.52311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1335.580000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.84858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.80135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1236.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.54051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.264200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.62249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1884.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.05819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.97226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.84949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.89496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.839700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.53139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>722.746400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.63135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163.169000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.19716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.45634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.42371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>509.371600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.56085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.05630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.58821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.41255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>732.945600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.42784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.904907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.80987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.919910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.57790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.20757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.23266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.85515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>464.239100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.31460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1225.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.63544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.40905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.04582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1668.995000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.97231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1147.941000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.23266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.95113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1461.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.43533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1117.189000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.10507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>989.476600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.08808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1559.213000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.21079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1904.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.32463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1948.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.53736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1232.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.44985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.973900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.76880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.41023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>760.569300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.83756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1874.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.07924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.36909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.27603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1846.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.08096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1845.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1638 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2         3    4    5       lenfol  fstat\n",
       "0     1.0  84.0  0.0  22.44662  1.0  1.0     7.589239      1\n",
       "1     0.0  36.0  0.0  31.64232  0.0  1.0  1266.000000      0\n",
       "2     0.0  76.0  0.0  46.48160  0.0  1.0  1453.000000      0\n",
       "3     0.0  47.0  1.0  27.58821  1.0  0.0   608.000000      0\n",
       "4     0.0  80.0  0.0  24.12942  1.0  0.0    22.616980      1\n",
       "5     0.0  77.0  0.0  23.67519  0.0  0.0  1313.000000      0\n",
       "6     0.0  90.0  1.0  27.46412  1.0  0.0     1.423138      1\n",
       "7     0.0  80.0  1.0  29.26457  1.0  0.0  1567.236000      1\n",
       "8     0.0  40.0  0.0  25.46070  0.0  0.0  1349.000000      0\n",
       "9     0.0  54.0  1.0  24.41255  1.0  1.0   278.901600      1\n",
       "10    0.0  81.0  0.0  28.48131  1.0  1.0  1965.911000      1\n",
       "11    0.0  75.0  0.0  29.27114  1.0  1.0  1929.816000      1\n",
       "12    0.0  63.0  0.0  39.22902  0.0  0.0  1999.000000      0\n",
       "13    0.0  79.0  0.0  24.52311  0.0  0.0  1335.580000      1\n",
       "14    0.0  80.0  0.0  25.84858  1.0  0.0   373.000000      0\n",
       "15    0.0  73.0  1.0  27.80135  0.0  0.0  1236.000000      0\n",
       "16    0.0  58.0  0.0  25.54051  0.0  0.0   162.264200      1\n",
       "17    0.0  73.0  1.0  28.62249  0.0  1.0  1884.000000      0\n",
       "18    0.0  58.0  0.0  31.05819  0.0  1.0  1114.000000      0\n",
       "19    0.0  80.0  1.0  17.97226  0.0  1.0   641.000000      0\n",
       "20    0.0  60.0  0.0  30.84949  0.0  0.0   469.000000      0\n",
       "21    1.0  86.0  1.0  18.89496  1.0  0.0   377.839700      1\n",
       "22    0.0  75.0  0.0  26.44901  0.0  1.0  1423.000000      0\n",
       "23    0.0  77.0  1.0  28.53139  1.0  0.0   722.746400      1\n",
       "24    0.0  41.0  0.0  26.44693  0.0  0.0  1988.000000      0\n",
       "25    0.0  90.0  1.0  23.63135  0.0  0.0  1163.169000      1\n",
       "26    0.0  58.0  0.0  27.19716  0.0  0.0   412.000000      0\n",
       "27    0.0  79.0  1.0  21.45634  0.0  0.0   617.000000      0\n",
       "28    0.0  65.0  1.0  49.42371  1.0  0.0   509.371600      1\n",
       "29    0.0  75.0  1.0  26.56085  0.0  0.0  1396.000000      0\n",
       "...   ...   ...  ...       ...  ...  ...          ...    ...\n",
       "1608  0.0  71.0  0.0  23.05630  0.0  0.0  1464.000000      0\n",
       "1609  0.0  47.0  1.0  27.58821  1.0  0.0   608.000000      0\n",
       "1610  0.0  88.0  1.0  24.41255  0.0  1.0   732.945600      1\n",
       "1611  1.0  50.0  0.0  32.42784  0.0  0.0     4.904907      1\n",
       "1612  1.0  92.0  0.0  21.80987  0.0  1.0    34.919910      1\n",
       "1613  1.0  77.0  0.0  19.57790  0.0  0.0     0.660839      1\n",
       "1614  0.0  68.0  1.0  21.20757  0.0  0.0   545.000000      0\n",
       "1615  0.0  72.0  1.0  25.23266  0.0  0.0  1860.000000      0\n",
       "1616  1.0  86.0  1.0  19.85515  0.0  0.0   464.239100      1\n",
       "1617  0.0  80.0  1.0  22.31460  0.0  0.0  1225.000000      0\n",
       "1618  0.0  53.0  0.0  31.63544  0.0  1.0  1920.000000      0\n",
       "1619  0.0  67.0  0.0  27.40905  0.0  0.0   532.000000      0\n",
       "1620  0.0  95.0  1.0  19.04582  1.0  0.0  1668.995000      1\n",
       "1621  0.0  43.0  1.0  25.97231  0.0  1.0  1147.941000      1\n",
       "1622  0.0  72.0  1.0  25.23266  0.0  0.0  1860.000000      0\n",
       "1623  0.0  45.0  1.0  31.95113  0.0  1.0  1461.000000      0\n",
       "1624  1.0  76.0  0.0  22.43533  1.0  0.0  1117.189000      1\n",
       "1625  0.0  42.0  0.0  25.10507  1.0  1.0   989.476600      1\n",
       "1626  0.0  65.0  0.0  33.08808  1.0  1.0  1559.213000      1\n",
       "1627  0.0  73.0  0.0  24.21079  0.0  0.0  1904.000000      0\n",
       "1628  0.0  52.0  0.0  31.32463  0.0  1.0  1948.000000      0\n",
       "1629  0.0  78.0  1.0  27.53736  0.0  0.0  1232.000000      0\n",
       "1630  0.0  76.0  0.0  37.44985  0.0  1.0   585.973900      1\n",
       "1631  0.0  64.0  0.0  25.76880  0.0  0.0  1880.000000      0\n",
       "1632  0.0  63.0  0.0  28.41023  1.0  0.0   760.569300      1\n",
       "1633  0.0  77.0  0.0  29.83756  0.0  0.0  1874.000000      0\n",
       "1634  0.0  48.0  0.0  33.07924  0.0  1.0   556.000000      0\n",
       "1635  0.0  81.0  0.0  27.36909  0.0  1.0   511.000000      0\n",
       "1636  0.0  83.0  0.0  24.27603  0.0  0.0  1846.000000      0\n",
       "1637  0.0  77.0  1.0  23.08096  0.0  0.0  1845.000000      0\n",
       "\n",
       "[1638 rows x 8 columns]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnagpal/anaconda2/lib/python2.7/site-packages/lifelines/utils/__init__.py:900: ConvergenceWarning: Column 0 have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      ">>> events = df['fstat'].astype(bool)\n",
      ">>> df.loc[events, '0'].var()\n",
      ">>> df.loc[~events, '0'].var()\n",
      "\n",
      "Too low variance here means that the column 0 completely determines whether a subject dies or not.\n",
      "See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression \n",
      "  warnings.warn(warning_text, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 1638 observations, 948 censored>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoxPHFitter().fit(ds.iloc[], 'lenfol', 'fstat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = ds['fstat'].astype(bool)\n",
    "ds.loc[events, '0'].var()\n",
    "ds.loc[~events, '0'].var()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
