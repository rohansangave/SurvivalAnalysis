{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lifelines.utils import concordance_index \n",
    "import sys\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import network\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAS = 1\n",
    "# GBSG = 2\n",
    "# METABRIC = 3\n",
    "# SUPPORT = 4\n",
    "\n",
    "DATASET_CHOICE = 3\n",
    "\n",
    "if (DATASET_CHOICE == 1):\n",
    "    # WHAS\n",
    "    ds = pd.read_csv('./datasets/whas1638.csv',sep=',')\n",
    "    train = ds[:1310]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[1310:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5']].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5']].as_matrix() \n",
    "elif (DATASET_CHOICE == 2):\n",
    "    # GBSG\n",
    "    ds = pd.read_csv('./datasets/gbsg2232.csv',sep=',')\n",
    "    train = ds[:1546]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[1546:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5', '6']].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5', '6']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5', '6']].as_matrix() \n",
    "elif (DATASET_CHOICE == 3):\n",
    "    # for METABRIC\n",
    "    ds = pd.read_csv('./datasets/metabric1904.csv',sep=',')\n",
    "    train = ds[:1523]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[1523:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix() \n",
    "elif (DATASET_CHOICE == 4):\n",
    "    # for SUPPORT\n",
    "    ds = pd.read_csv('./datasets/support8873.csv',sep=',')\n",
    "    train = ds[:7098]\n",
    "    valid = train[-100:]\n",
    "    train = train[:-100]\n",
    "    test = ds[7098:]\n",
    "    x_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', ]].as_matrix()\n",
    "    x_valid = valid[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix()\n",
    "    x_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "x_train = scl.fit_transform(x_train)\n",
    "\n",
    "e_train = train['fstat']\n",
    "t_train = train['lenfol']\n",
    "\n",
    "\n",
    "x_valid = scl.fit_transform(x_valid)\n",
    "\n",
    "e_valid = valid['fstat']\n",
    "t_valid = valid['lenfol']\n",
    "\n",
    "\n",
    "x_test = scl.transform(x_test)\n",
    "\n",
    "e_test = test['fstat']\n",
    "t_test = test['lenfol']\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "e_train = torch.from_numpy(e_train.as_matrix()).float()\n",
    "t_train = torch.from_numpy(t_train.as_matrix())\n",
    "\n",
    "x_valid = torch.from_numpy(x_valid).float()\n",
    "e_valid = torch.from_numpy(e_valid.as_matrix()).float()\n",
    "t_valid = torch.from_numpy(t_valid.as_matrix())\n",
    "\n",
    "\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "e_test = torch.from_numpy(e_test.as_matrix()).float()\n",
    "t_test = torch.from_numpy(t_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1423, 9])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1904, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>lenfol</th>\n",
       "      <th>fstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.603834</td>\n",
       "      <td>7.811392</td>\n",
       "      <td>10.797988</td>\n",
       "      <td>5.967607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.84</td>\n",
       "      <td>99.333336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.284882</td>\n",
       "      <td>9.581043</td>\n",
       "      <td>10.204620</td>\n",
       "      <td>5.664970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.94</td>\n",
       "      <td>95.733330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.920251</td>\n",
       "      <td>6.776564</td>\n",
       "      <td>12.431715</td>\n",
       "      <td>5.873857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.44</td>\n",
       "      <td>140.233340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.654017</td>\n",
       "      <td>5.341845</td>\n",
       "      <td>8.646379</td>\n",
       "      <td>5.655888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.91</td>\n",
       "      <td>239.300000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.456747</td>\n",
       "      <td>5.339741</td>\n",
       "      <td>10.555724</td>\n",
       "      <td>6.008429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.85</td>\n",
       "      <td>56.933334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1          2         3    4    5    6    7      8  \\\n",
       "0  5.603834  7.811392  10.797988  5.967607  1.0  1.0  0.0  1.0  56.84   \n",
       "1  5.284882  9.581043  10.204620  5.664970  1.0  0.0  0.0  1.0  85.94   \n",
       "2  5.920251  6.776564  12.431715  5.873857  0.0  1.0  0.0  1.0  48.44   \n",
       "3  6.654017  5.341845   8.646379  5.655888  0.0  0.0  0.0  0.0  66.91   \n",
       "4  5.456747  5.339741  10.555724  6.008429  1.0  0.0  0.0  1.0  67.85   \n",
       "\n",
       "       lenfol  fstat  \n",
       "0   99.333336      0  \n",
       "1   95.733330      1  \n",
       "2  140.233340      0  \n",
       "3  239.300000      0  \n",
       "4   56.933334      1  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    x_train = x_train.cuda()\n",
    "    e_train = e_train.cuda()\n",
    "    t_train = t_train.cuda()\n",
    "    x_valid = x_valid.cuda()\n",
    "    e_valid = e_valid.cuda()\n",
    "    t_valid = t_valid.cuda()\n",
    "    x_test = x_test.cuda()\n",
    "    e_test = e_test.cuda()\n",
    "    e_test = t_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting risk set computation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541865a81ac044578444ffc1aabdaee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1423), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3614fd1f6f9949548a4886d9fb57896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4c4dfaa6654fd5975db743e2dc40e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=381), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk set computed\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "#         m.weight.data.fill_(0)\n",
    "#         m.bias.data.fill_(1)\n",
    "\n",
    "def init_weights_for_cox(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "t_ = t_train.cpu().data.numpy()\n",
    "        \n",
    "print(\"starting risk set computation...\")\n",
    "risk_set = []\n",
    "for i in tqdm(range(len(t_))):\n",
    "\n",
    "    risk_set.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "t_ = t_valid.cpu().data.numpy()\n",
    "        \n",
    "risk_set_valid = []\n",
    "for i in tqdm(range(len(t_))):\n",
    "\n",
    "    risk_set_valid.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "    \n",
    "    \n",
    "t_ = t_test.cpu().data.numpy()\n",
    "\n",
    "risk_set_test = []\n",
    "for i in tqdm(range(len(t_test))):\n",
    "\n",
    "    risk_set_test.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "print(\"risk set computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1423, 9])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1421\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1422\n",
      "1418\n",
      "1391\n",
      "1415\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1414\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1414\n",
      "1421\n",
      "1414\n",
      "1422\n",
      "1421\n",
      "1391\n",
      "1415\n",
      "1405\n",
      "1405\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1421\n",
      "1414\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1244\n",
      "1418\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1415\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1414\n",
      "1421\n",
      "1414\n",
      "1418\n",
      "1415\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1391\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1391\n",
      "1414\n",
      "1422\n",
      "1418\n",
      "1418\n",
      "1421\n",
      "1422\n",
      "1391\n",
      "1415\n",
      "1391\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1391\n",
      "1391\n",
      "1422\n",
      "1405\n",
      "1421\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1421\n",
      "1244\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1411\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1411\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1244\n",
      "1418\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1421\n",
      "1411\n",
      "1415\n",
      "1422\n",
      "1405\n",
      "1418\n",
      "1405\n",
      "1415\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1414\n",
      "1391\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1411\n",
      "1391\n",
      "1414\n",
      "1244\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1418\n",
      "1418\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1421\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1414\n",
      "1414\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1411\n",
      "1415\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1414\n",
      "1414\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1244\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1244\n",
      "1405\n",
      "1414\n",
      "1422\n",
      "1418\n",
      "1415\n",
      "1405\n",
      "1405\n",
      "1421\n",
      "1411\n",
      "1418\n",
      "1418\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1391\n",
      "1405\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1411\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1418\n",
      "1415\n",
      "1418\n",
      "1415\n",
      "1418\n",
      "1418\n",
      "1414\n",
      "1405\n",
      "1411\n",
      "1422\n",
      "1244\n",
      "1411\n",
      "1414\n",
      "1405\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1415\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1414\n",
      "1391\n",
      "1422\n",
      "1415\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1405\n",
      "1414\n",
      "1415\n",
      "1418\n",
      "1405\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1244\n",
      "1391\n",
      "1418\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1411\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1405\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1415\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1418\n",
      "1411\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1418\n",
      "1418\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1411\n",
      "1422\n",
      "1414\n",
      "1411\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1421\n",
      "1415\n",
      "1405\n",
      "1414\n",
      "1405\n",
      "1391\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1418\n",
      "1411\n",
      "1422\n",
      "1421\n",
      "1391\n",
      "1405\n",
      "1421\n",
      "1418\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1411\n",
      "1418\n",
      "1405\n",
      "1418\n",
      "1421\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1414\n",
      "1421\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1411\n",
      "1405\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1405\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1418\n",
      "1422\n",
      "1411\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1414\n",
      "1421\n",
      "1418\n",
      "1405\n",
      "1244\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1421\n",
      "1244\n",
      "1411\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1418\n",
      "1391\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1405\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1411\n",
      "1405\n",
      "1421\n",
      "1411\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1244\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1405\n",
      "1415\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1422\n",
      "1418\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1391\n",
      "1405\n",
      "1411\n",
      "1414\n",
      "1405\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1391\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1391\n",
      "1415\n",
      "1405\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1415\n",
      "1391\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1418\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1421\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1418\n",
      "1414\n",
      "1244\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1405\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1418\n",
      "1418\n",
      "1414\n",
      "1405\n",
      "1391\n",
      "1418\n",
      "1411\n",
      "1415\n",
      "1244\n",
      "1391\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1418\n",
      "1418\n",
      "1422\n",
      "1418\n",
      "1418\n",
      "1422\n",
      "1418\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1405\n",
      "1244\n",
      "1421\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1418\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1415\n",
      "1414\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1418\n",
      "1422\n",
      "1411\n",
      "1415\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1418\n",
      "1391\n",
      "1421\n",
      "1422\n",
      "1391\n",
      "1418\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1244\n",
      "1418\n",
      "1415\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1244\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1415\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1414\n",
      "1411\n",
      "1415\n",
      "1422\n",
      "1411\n",
      "1405\n",
      "1411\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1391\n",
      "1415\n",
      "1414\n",
      "1418\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1411\n",
      "1405\n",
      "1405\n",
      "1405\n",
      "1405\n",
      "1421\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1414\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1411\n",
      "1418\n",
      "1414\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1421\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1421\n",
      "1418\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1411\n",
      "1405\n",
      "1411\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1391\n",
      "1421\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1244\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1411\n",
      "1414\n",
      "1405\n",
      "1244\n",
      "1414\n",
      "1415\n",
      "1422\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1405\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1414\n",
      "1411\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1405\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1418\n",
      "1421\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1391\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1414\n",
      "1415\n",
      "1418\n",
      "1391\n",
      "1418\n",
      "1411\n",
      "1422\n",
      "1411\n",
      "1418\n",
      "1391\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1418\n",
      "1414\n",
      "1415\n",
      "1414\n",
      "1422\n",
      "1415\n",
      "1405\n",
      "1418\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1244\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1421\n",
      "1391\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1405\n",
      "1422\n",
      "1414\n",
      "1421\n",
      "1405\n",
      "1411\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1415\n",
      "1415\n",
      "1418\n",
      "1422\n",
      "1418\n",
      "1421\n",
      "1414\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1411\n",
      "1405\n",
      "1421\n",
      "1391\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1405\n",
      "1405\n",
      "1391\n",
      "1405\n",
      "1415\n",
      "1422\n",
      "1244\n",
      "1422\n",
      "1418\n",
      "1418\n",
      "1414\n",
      "1414\n",
      "1421\n",
      "1415\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1418\n",
      "1421\n",
      "1422\n",
      "1414\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1405\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1415\n",
      "1391\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1391\n",
      "1414\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1414\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1418\n",
      "1421\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1391\n",
      "1411\n",
      "1421\n",
      "1422\n",
      "1418\n",
      "1411\n",
      "1414\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1415\n",
      "1421\n",
      "1411\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1418\n",
      "1405\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1421\n",
      "1422\n",
      "1414\n",
      "1418\n",
      "1391\n",
      "1422\n",
      "1415\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1391\n",
      "1421\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1415\n",
      "1411\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1421\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1415\n",
      "1418\n",
      "1414\n",
      "1391\n",
      "1422\n",
      "1421\n",
      "1411\n",
      "1405\n",
      "1414\n",
      "1414\n",
      "1405\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1415\n",
      "1418\n",
      "1418\n",
      "1405\n",
      "1418\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1391\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1414\n",
      "1422\n",
      "1411\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1422\n",
      "1405\n",
      "1422\n",
      "1422\n",
      "1422\n",
      "1414\n",
      "1422\n",
      "1411\n",
      "1414\n",
      "1422\n",
      "1414\n",
      "1415\n",
      "1422\n",
      "1422\n",
      "1418\n",
      "1422\n",
      "1422\n",
      "1421\n",
      "1422\n"
     ]
    }
   ],
   "source": [
    "for r in risk_set:\n",
    "    if max(r)>1210:\n",
    "        print(max(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def elbo(risk, gated_output, E, risk_set, CUDA):\n",
    "#     lgo_sm = nn.LogSoftmax(dim=1)(gated_output)\n",
    "#     lnumerator = torch.mul(torch.exp(lgo_sm), risk)\n",
    "    \n",
    "#     lnumerator = torch.sum(lnumerator, dim=1)\n",
    "    \n",
    "#     expected_risks = risk + lgo_sm\n",
    "#     expected_risks = torch.logsumexp(expected_risks, dim=1)\n",
    "#     ldenominator = []\n",
    "#     for i in range(risk.shape[0]):\n",
    "#         ldenominator.append(torch.logsumexp(expected_risks[risk_set[i]], dim=0))\n",
    "        \n",
    "#     ldenominator = torch.stack(ldenominator, dim=0)\n",
    "#    # ldenominator = torch.log(ldenominator)\n",
    "    \n",
    "#     likelihoods = lnumerator - ldenominator\n",
    "    \n",
    "#     E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# #     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "#     likelihoods = likelihoods[E]\n",
    "#     neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "#     return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(risk, gated_output, E, risk_set, CUDA):\n",
    "    go_sm = nn.Softmax(dim=1)(gated_output)\n",
    "    lnumerator = torch.mul(go_sm, risk)\n",
    "    \n",
    "    lnumerator = torch.sum(lnumerator, dim=1)\n",
    "    \n",
    "    expected_risks = torch.exp(risk) * go_sm\n",
    "    expected_risks = torch.sum(expected_risks, dim=1)\n",
    "    ldenominator = []\n",
    "    for i in range(risk.shape[0]):\n",
    "        ldenominator.append(torch.sum(expected_risks[risk_set[i]], dim=0))\n",
    "        \n",
    "    ldenominator = torch.stack(ldenominator, dim=0)\n",
    "    ldenominator = torch.log(ldenominator)\n",
    "    \n",
    "    likelihoods = lnumerator - ldenominator\n",
    "    \n",
    "    E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "#     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "    likelihoods = likelihoods[E]\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to change code so that test runs after model is trained\n",
    "def train(gated_network, betas_network, risk_set, x_train, e_train, t_train, risk_set_valid, x_valid, e_valid, t_valid, CUDA, \n",
    "          optimizer, n_epochs,x_test,e_test,t_test,risk_set_test):\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    # Initialize Metrics\n",
    "    c_index_soft = []\n",
    "    c_index_hard = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    test_loss = []\n",
    "    test_c_index_soft = []\n",
    "    test_c_index_hard = []\n",
    "    diff = 1e-4\n",
    "    prev_loss_train = 0\n",
    "    prev_loss_valid = 0\n",
    "    bad_cnt = 0\n",
    "    start = time.time()\n",
    "      \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        gated_network.train()\n",
    "        betas_network.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        gated_outputs = gated_network(x_train)\n",
    "        lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "        betas_output = betas_network(x_train)\n",
    "        \n",
    "        ci_train_soft,ci_train_hard = get_concordance_index(betas_output, gated_outputs, t_train, e_train, bootstrap=False)\n",
    "        c_index_soft.append(ci_train_soft)\n",
    "        c_index_hard.append(ci_train_hard)\n",
    "        \n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss = elbo(betas_output, gated_outputs, e_train, risk_set, CUDA) + (betas_network[0].weight**2).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        my_loss = loss.cpu().data.numpy()\n",
    "        train_loss.append(my_loss)\n",
    "        if abs(my_loss - prev_loss_train) < diff:\n",
    "            break\n",
    "        prev_loss_train = my_loss\n",
    "        \n",
    "        \n",
    "        torch.cuda.empty_cache()          \n",
    "        \n",
    "        \n",
    "        ################################################# Validation #######################################################\n",
    "        gated_network.eval()\n",
    "        betas_network.eval()\n",
    "\n",
    "        gated_outputs_valid = gated_network(x_valid)\n",
    "        lsoftmax_valid = nn.LogSoftmax(dim=1)(gated_outputs_valid)\n",
    "        \n",
    "        betas_output_valid = betas_network(x_valid)\n",
    "              \n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss_valid = elbo(betas_output_valid, gated_outputs_valid, e_valid, risk_set_valid, CUDA)\n",
    "        \n",
    "\n",
    "        my_loss_valid = loss_valid.cpu().data.numpy()\n",
    "        valid_loss.append(my_loss_valid)\n",
    "        if my_loss_valid - prev_loss_valid > diff:\n",
    "            bad_cnt+=1\n",
    "            if bad_cnt>2:\n",
    "                break\n",
    "        else:\n",
    "            bad_cnt=0\n",
    "        prev_loss_valid = my_loss_valid\n",
    "        \n",
    "        torch.cuda.empty_cache()          \n",
    "\n",
    "        \n",
    "        ################################################# Test #############################################################\n",
    "    gated_network.eval()\n",
    "    betas_network.eval()\n",
    "\n",
    "\n",
    "    gated_outputs_test = gated_network(x_test)\n",
    "    lsoftmax_test = nn.LogSoftmax(dim=1)(gated_outputs_test)\n",
    "\n",
    "    betas_output_test = betas_network(x_test)\n",
    "\n",
    "    ci_test_soft,ci_test_hard = get_concordance_index(betas_output_test, gated_outputs_test, t_test, e_test, bootstrap=250)\n",
    "    test_c_index_soft.append(ci_test_soft)\n",
    "    test_c_index_hard.append(ci_test_hard)\n",
    "\n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "    loss_test = elbo(betas_output_test, gated_outputs_test, e_test, risk_set_test, CUDA)\n",
    "\n",
    "\n",
    "    my_loss_test = loss_test.cpu().data.numpy()\n",
    "    test_loss.append(loss_test)\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()          \n",
    "    #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "\n",
    "        \n",
    "    metrics = {}\n",
    "    metrics['train_loss'] = train_loss\n",
    "    metrics['valid_loss'] = valid_loss\n",
    "\n",
    "    metrics['c-index-soft'] = c_index_soft\n",
    "    metrics['c-index-hard'] = c_index_hard\n",
    "    metrics['test_loss'] = test_loss\n",
    "    metrics['c-index-test-soft'] = test_c_index_soft\n",
    "    metrics['c-index-test-hard'] = test_c_index_hard\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo_batch(risk, gated_output, E, risk_set, CUDA):\n",
    "    \n",
    "\n",
    "    go_sm = nn.Softmax(dim=1)(gated_output)\n",
    "    lnumerator = torch.mul(go_sm[0], risk[0])\n",
    "\n",
    "#     lnumerator = torch.sum(lnumerator, dim=1)\n",
    "\n",
    "    expected_risks = torch.exp(risk) * go_sm\n",
    "    expected_risks = torch.sum(expected_risks, dim=1)\n",
    "\n",
    "    ldenominator = torch.sum(expected_risks, dim=0)\n",
    "\n",
    "    ldenominator = torch.log(ldenominator)\n",
    "\n",
    "    likelihoods = lnumerator - ldenominator\n",
    "\n",
    "\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "\n",
    "    \n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to fix for hard and soft gating\n",
    "def train_batch(gated_network, betas_network, risk_set, x, e, t, CUDA, optimizer, n_epochs,x_valid,e_valid,t_valid,risk_set_validation):\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    # Initialize Metrics\n",
    "    c_index = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    valid_c_index = []\n",
    "    diff = 1e-4\n",
    "    \n",
    "    prev_loss_train = 0\n",
    "    prev_loss_valid = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        eloss = 0\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            \n",
    "            if e[i] == 0:\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # print(\"x: \", x)\n",
    "            gated_outputs = gated_network(x[risk_set[i]])\n",
    "            lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "\n",
    "            betas_output = betas_network(x[risk_set[i]])\n",
    "            \n",
    "\n",
    "    #         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "            loss = elbo_batch(betas_output, gated_outputs, e[i], risk_set, CUDA)\n",
    "                        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            torch.cuda.empty_cache() \n",
    "            \n",
    "            eloss+=loss.cpu().data.numpy()\n",
    "\n",
    "        gated_outputs = gated_network(x)\n",
    "        lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "        \n",
    "        betas_output = betas_network(x)\n",
    "        \n",
    "        loss_train = elbo(betas_output, gated_outputs, e, risk_set, CUDA)\n",
    "\n",
    "        \n",
    "        ci_train = get_concordance_index(betas_output, gated_outputs, t, e)\n",
    "        c_index.append(ci_train)\n",
    "    \n",
    "            #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "\n",
    "        gated_outputs_valid = gated_network(x_valid)\n",
    "        lsoftmax_valid = nn.LogSoftmax(dim=1)(gated_outputs_valid)\n",
    "\n",
    "        betas_output_valid = betas_network(x_valid)\n",
    "\n",
    "        ci_valid = get_concordance_index(betas_output_valid, gated_outputs_valid, t_valid, e_valid)\n",
    "        valid_c_index.append(ci_valid)\n",
    "\n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss_valid = elbo(betas_output_valid, gated_outputs_valid, e_valid, risk_set_validation, CUDA)\n",
    "\n",
    "\n",
    "        my_loss_valid = loss_valid.cpu().data.numpy()\n",
    "        valid_loss.append(my_loss_valid)\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()          \n",
    "            #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "        \n",
    "        train_loss.append(eloss)\n",
    "        if abs(eloss - prev_loss) < diff:\n",
    "            break\n",
    "        prev_loss = eloss\n",
    "\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['train_loss'] = train_loss\n",
    "    metrics['c-index'] = c_index\n",
    "    metrics['valid_loss'] = valid_loss\n",
    "    metrics['c-index-valid'] = valid_c_index\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(risk, lsoftmax, E, risk_set, CUDA):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "#     new_risk = []\n",
    "#     for i in range(len(risk_set)):\n",
    "#         new_risk.append(risk[risk_set[i]])\n",
    "        \n",
    "#     log_risk = []\n",
    "#     for i in range(len(new_risk)):\n",
    "#         temp = torch.logsumexp(new_risk[i], 0)\n",
    "#         log_risk.append(temp)\n",
    "\n",
    "    lnumerator = risk\n",
    "    \n",
    "    idxs = range(risk.shape[0])\n",
    "    \n",
    "    \n",
    "    ldenominator = []\n",
    "    \n",
    "    for i in range(len(idxs)):\n",
    "        ldenominator.append(torch.logsumexp(risk[risk_set[i]], dim=0))\n",
    "            \n",
    "                            \n",
    "    ldenominator = torch.stack(ldenominator, dim=0)\n",
    "    print(ldenominator.shape)\n",
    "#     print(lnumerator.shape)\n",
    "    \n",
    "    \n",
    "    likelihoods = lnumerator - ldenominator\n",
    "    print(likelihoods.shape)\n",
    "    \n",
    "    E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "#     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "    likelihoods = likelihoods[E] + lsoftmax[E]\n",
    "    likelihoods = torch.logsumexp(likelihoods, dim=1)\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "\n",
    "    return neg_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concordance_index(x, gated_x, t, e, bootstrap=False):\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.utils import resample\n",
    "#     x = x.detach().cpu().numpy()\n",
    "    t = t.detach().cpu().numpy()\n",
    "    e = e.detach().cpu().numpy()\n",
    "    softmax = torch.nn.Softmax(dim=1)(gated_x)\n",
    "    \n",
    "    r = x.shape[0]\n",
    "    \n",
    "    soft_computed_hazard = torch.exp(x)\n",
    "    hard_computed_hazard = soft_computed_hazard[range(r),gated_x.argmax(1)[1]]\n",
    "\n",
    "    \n",
    "    soft_computed_hazard = torch.mul(softmax, soft_computed_hazard)\n",
    "    soft_computed_hazard = torch.sum(soft_computed_hazard, dim = 1)\n",
    "    \n",
    "    soft_computed_hazard = -1*soft_computed_hazard.detach().cpu().numpy()\n",
    "    \n",
    "    hard_computed_hazard = -1*hard_computed_hazard.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "\n",
    "    if not bootstrap:\n",
    "    \n",
    "        return concordance_index(t,soft_computed_hazard,e),concordance_index(t,hard_computed_hazard,e)\n",
    "\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        soft_concord, hard_concord = [], []\n",
    "        \n",
    "        for i in range(bootstrap):\n",
    "            \n",
    "            soft_dat_, e_, t_ = resample(soft_computed_hazard, e, t )        \n",
    "\n",
    "            sci = concordance_index(t_,soft_dat_,e_)\n",
    "            \n",
    "            hard_dat_,  e_, t_  = resample(hard_computed_hazard,  e, t )\n",
    "       \n",
    "            hci = concordance_index(t_,hard_dat_,e_)\n",
    "            \n",
    "            soft_concord.append(sci)\n",
    "            hard_concord.append(hci)\n",
    "\n",
    "            \n",
    "        return soft_concord, hard_concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-137-e4c390e49237>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-137-e4c390e49237>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    layers_sizes = [n_in  k]\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_in = x_train.shape[1]\n",
    "k = 20\n",
    "\n",
    "betas_network = nn.Sequential(nn.Linear(n_in, k, bias=False))#, nn.ReLU(),nn.Linear(20, 20, bias=False), nn.ReLU(),nn.Linear(20, k, bias=False))\n",
    "#betas_network.apply(init_weights)\n",
    "\n",
    "# Construct Neural Network\n",
    "layers_sizes = [n_in,  k]\n",
    "layers = []\n",
    "for i in range(len(layers_sizes)-2):\n",
    "    layers.append(nn.Linear(layers_sizes[i],layers_sizes[i+1],bias=False ))\n",
    "    layers.append(nn.ReLU())\n",
    "\n",
    "layers.append(nn.Linear(layers_sizes[-2], layers_sizes[-1], bias=False))\n",
    "gated_network = nn.Sequential(*layers)\n",
    "#gated_network.apply(init_weights)\n",
    "\n",
    "#optimizer = optimizer = torch.optim.SGD(my_network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=L2_reg, nesterov=True)\n",
    "optimizer = torch.optim.Adam(list(gated_network.parameters()) + list(betas_network.parameters()), lr=1e-4)\n",
    "betas_network.train()\n",
    "gated_network.train()\n",
    "\n",
    "if CUDA:\n",
    "    gated_network.cuda()\n",
    "    betas_network.cuda()\n",
    "\n",
    "# If you have validation data, you can add it as the valid_dataloader parameter to the function\n",
    "n_epochs = 2000\n",
    "metrics = train(gated_network, betas_network, risk_set, x_train, e_train, t_train,  risk_set_valid, x_valid, e_valid, t_valid, CUDA, optimizer, n_epochs,x_test,e_test,t_test,risk_set_test)\n",
    "print() \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6379621972325714\n",
      "0.6378527846219866\n",
      "0.6334703171784373\n",
      "0.6326292806818234\n"
     ]
    }
   ],
   "source": [
    "print(metrics['c-index-soft'][-1])\n",
    "print(metrics['c-index-hard'][-1])\n",
    "print(np.mean(metrics['c-index-test-soft'][-1]))\n",
    "print(np.mean(metrics['c-index-test-hard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013699920664076272\n",
      "0.018040465316470425\n"
     ]
    }
   ],
   "source": [
    "print(np.std(metrics['c-index-test-soft'][-1])/np.sqrt(250) )\n",
    "print(np.std(metrics['c-index-test-hard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(655.7275, grad_fn=<NegBackward>)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfec71e980ef48b9825072f4d59d636f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 2\n",
      "Learning Rate: 0.0001\n",
      "0.7255547232830286\n",
      "0.6858012942519984\n",
      "0.5873404474013717\n",
      "0.5765759031191015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb6a0b7b7ce43669b05d8c41df865a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 2\n",
      "Learning Rate: 0.001\n",
      "0.4040571239117354\n",
      "0.2796075493940101\n",
      "0.4559606795118578\n",
      "0.5238744884038199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f519c382e327427fb9c249bae0b6e464"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 5\n",
      "Learning Rate: 0.0001\n",
      "0.72298832226138\n",
      "0.7192799341822513\n",
      "0.5957689360668299\n",
      "0.5680726606739053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a8b1b308f742fab76755164ab5cf44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 5\n",
      "Learning Rate: 0.001\n",
      "0.6618318454756438\n",
      "0.6204968257671575\n",
      "0.5651946401539928\n",
      "0.5577379506251284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd916c7ef42a4776b8ad4ebf5010c525"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 7\n",
      "Learning Rate: 0.0001\n",
      "0.6767046919704802\n",
      "0.49711556172255855\n",
      "0.5864994673793194\n",
      "0.49879459530172493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46fb8770eba49e2a0c5685199585e62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 7\n",
      "Learning Rate: 0.001\n",
      "0.6336284489851051\n",
      "0.6898756093667497\n",
      "0.6007026855295371\n",
      "0.5790054009605861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50d4d8a24104b46a14f3ce112325390"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 10\n",
      "Learning Rate: 0.0001\n",
      "0.5495831133268662\n",
      "0.3088866240161106\n",
      "0.5323216655142126\n",
      "0.5483376628230765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbf3222034c402fb3759b7e58a6c3a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models: 10\n",
      "Learning Rate: 0.001\n",
      "0.33140955585299064\n",
      "0.25734002971622233\n",
      "0.4249752378104618\n",
      "0.5030555607467903\n",
      "Best Concordance for hard gating: 0.5790054009605861 with parameters:\n",
      "Learning rate- 0.001\n",
      "Linear models- 7\n",
      "Best Concordance for hard gating: 0.6007026855295371 with parameters:\n",
      "Learning rate- 0.001\n",
      "Linear models- 7\n"
     ]
    }
   ],
   "source": [
    "#grid search:\n",
    "\n",
    "linear_models=[2,5,7,10]\n",
    "learning_rates=[1e-4,1e-3]\n",
    "best_k_hard=0\n",
    "best_lr_hard=0\n",
    "best_k_soft=0\n",
    "best_lr_soft=0\n",
    "\n",
    "best_ci_soft=0\n",
    "best_ci_hard=0\n",
    "\n",
    "for k in linear_models:\n",
    "    for lr in learning_rates:\n",
    "        n_in = x_train.shape[1]\n",
    "        betas_network = nn.Sequential(nn.Linear(n_in, k, bias=False) )\n",
    "        betas_network.apply(init_weights)\n",
    "\n",
    "    # Construct Neural Network\n",
    "        layers_sizes = [n_in,20,20, k]\n",
    "        layers = []\n",
    "        for i in range(len(layers_sizes)-2):\n",
    "            layers.append(nn.Linear(layers_sizes[i],layers_sizes[i+1], ))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(layers_sizes[-2], layers_sizes[-1], bias=False))\n",
    "        gated_network = nn.Sequential(*layers)\n",
    "        gated_network.apply(init_weights)\n",
    "\n",
    "    #optimizer = optimizer = torch.optim.SGD(my_network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=L2_reg, nesterov=True)\n",
    "        optimizer = torch.optim.Adam(list(gated_network.parameters()) + list(betas_network.parameters()), lr=lr)\n",
    "    \n",
    "        if CUDA:\n",
    "            gated_network.cuda()\n",
    "            betas_network.cuda()\n",
    "\n",
    "    # If you have validation data, you can add it as the valid_dataloader parameter to the function\n",
    "        n_epochs = 1\n",
    "        metrics = train(gated_network, betas_network, risk_set, x_train, e_train, t_train,  risk_set_valid, x_valid, e_valid, t_valid, CUDA, optimizer, n_epochs,x_test,e_test,t_test,risk_set_test)\n",
    "        print() \n",
    "        print(\"Models:\",k)\n",
    "        print(\"Learning Rate:\",lr)\n",
    "    \n",
    "        print(metrics['c-index-soft'][-1])\n",
    "        print(metrics['c-index-hard'][-1])\n",
    "        print(metrics['c-index-test-soft'][-1])\n",
    "        print(metrics['c-index-test-hard'][-1])\n",
    "    \n",
    "        if metrics['c-index-test-soft'][-1]>best_ci_soft:\n",
    "            best_ci_soft=metrics['c-index-test-soft'][-1]\n",
    "            best_k_soft=k\n",
    "            best_lr_soft=lr\n",
    "\n",
    "        if metrics['c-index-test-hard'][-1]>best_ci_hard:\n",
    "            best_ci_hard=metrics['c-index-test-hard'][-1]\n",
    "            best_k_hard=k\n",
    "            best_lr_hard=lr\n",
    "        \n",
    "print('Best Concordance for hard gating:',best_ci_hard, \"with parameters:\" )\n",
    "print('Learning rate-',best_lr_hard)\n",
    "print('Linear models-',best_k_hard)\n",
    "\n",
    "print('Best Concordance for hard gating:',best_ci_soft, \"with parameters:\" )\n",
    "print('Learning rate-',best_lr_soft)\n",
    "print('Linear models-',best_k_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7847247969260844\n",
      "0.5149482624962082\n"
     ]
    }
   ],
   "source": [
    "print(np.max(metrics['c-index-valid-soft']))\n",
    "print(np.max(metrics['c-index-valid-hard']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('num of epochs: ', 3319)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'c-index-valid-soft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-21dbcceb5a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c-index-soft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c-index-soft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c-index-valid-soft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c-index-valid-soft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no. of epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'c-index-valid-soft'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the final metrics\n",
    "# print('Train C-Index:', metrics['c-index'])\n",
    "# print('Valid C-Index: ',metrics['valid_c-index'][-1])\n",
    "\n",
    "print(\"num of epochs: \", len(metrics['train_loss']))\n",
    "# print(metrics['train_loss'])\n",
    "# Plot the training / validation curves\n",
    "plt.plot(range(len(metrics['train_loss'])), metrics['train_loss'])\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('training loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(metrics['valid_loss'])), metrics['valid_loss'])\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('validation loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(len(metrics['c-index-soft'])), metrics['c-index-soft'])\n",
    "plt.plot(range(len(metrics['c-index-valid-soft'])), metrics['c-index-valid-soft'])\n",
    "\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('concordance index')\n",
    "# plt.plot(range(len(metrics['c-index'])), metrics['c-index'])\n",
    "plt.xscale('linear')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(len(metrics['c-index-hard'])), metrics['c-index-hard'])\n",
    "plt.plot(range(len(metrics['c-index-valid-hard'])), metrics['c-index-valid-hard'])\n",
    "\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('concordance index')\n",
    "# plt.plot(range(len(metrics['c-index'])), metrics['c-index'])\n",
    "plt.xscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>lenfol</th>\n",
       "      <th>fstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.44662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.589239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.64232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.48160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1453.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.58821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.12942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.616980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.67519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.46412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.423138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.26457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1567.236000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.46070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1349.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.41255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278.901600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.48131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1965.911000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.27114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1929.816000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.22902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.52311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1335.580000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.84858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.80135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1236.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.54051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.264200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.62249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1884.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.05819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.97226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.84949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.89496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.839700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.53139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>722.746400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.63135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163.169000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.19716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.45634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.42371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>509.371600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.56085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.05630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.58821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.41255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>732.945600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.42784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.904907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.80987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.919910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.57790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.20757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.23266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.85515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>464.239100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.31460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1225.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.63544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.40905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.04582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1668.995000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.97231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1147.941000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.23266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.95113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1461.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.43533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1117.189000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.10507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>989.476600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.08808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1559.213000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.21079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1904.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.32463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1948.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.53736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1232.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.44985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.973900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.76880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.41023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>760.569300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.83756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1874.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.07924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.36909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.27603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1846.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.08096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1845.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1638 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2         3    4    5       lenfol  fstat\n",
       "0     1.0  84.0  0.0  22.44662  1.0  1.0     7.589239      1\n",
       "1     0.0  36.0  0.0  31.64232  0.0  1.0  1266.000000      0\n",
       "2     0.0  76.0  0.0  46.48160  0.0  1.0  1453.000000      0\n",
       "3     0.0  47.0  1.0  27.58821  1.0  0.0   608.000000      0\n",
       "4     0.0  80.0  0.0  24.12942  1.0  0.0    22.616980      1\n",
       "5     0.0  77.0  0.0  23.67519  0.0  0.0  1313.000000      0\n",
       "6     0.0  90.0  1.0  27.46412  1.0  0.0     1.423138      1\n",
       "7     0.0  80.0  1.0  29.26457  1.0  0.0  1567.236000      1\n",
       "8     0.0  40.0  0.0  25.46070  0.0  0.0  1349.000000      0\n",
       "9     0.0  54.0  1.0  24.41255  1.0  1.0   278.901600      1\n",
       "10    0.0  81.0  0.0  28.48131  1.0  1.0  1965.911000      1\n",
       "11    0.0  75.0  0.0  29.27114  1.0  1.0  1929.816000      1\n",
       "12    0.0  63.0  0.0  39.22902  0.0  0.0  1999.000000      0\n",
       "13    0.0  79.0  0.0  24.52311  0.0  0.0  1335.580000      1\n",
       "14    0.0  80.0  0.0  25.84858  1.0  0.0   373.000000      0\n",
       "15    0.0  73.0  1.0  27.80135  0.0  0.0  1236.000000      0\n",
       "16    0.0  58.0  0.0  25.54051  0.0  0.0   162.264200      1\n",
       "17    0.0  73.0  1.0  28.62249  0.0  1.0  1884.000000      0\n",
       "18    0.0  58.0  0.0  31.05819  0.0  1.0  1114.000000      0\n",
       "19    0.0  80.0  1.0  17.97226  0.0  1.0   641.000000      0\n",
       "20    0.0  60.0  0.0  30.84949  0.0  0.0   469.000000      0\n",
       "21    1.0  86.0  1.0  18.89496  1.0  0.0   377.839700      1\n",
       "22    0.0  75.0  0.0  26.44901  0.0  1.0  1423.000000      0\n",
       "23    0.0  77.0  1.0  28.53139  1.0  0.0   722.746400      1\n",
       "24    0.0  41.0  0.0  26.44693  0.0  0.0  1988.000000      0\n",
       "25    0.0  90.0  1.0  23.63135  0.0  0.0  1163.169000      1\n",
       "26    0.0  58.0  0.0  27.19716  0.0  0.0   412.000000      0\n",
       "27    0.0  79.0  1.0  21.45634  0.0  0.0   617.000000      0\n",
       "28    0.0  65.0  1.0  49.42371  1.0  0.0   509.371600      1\n",
       "29    0.0  75.0  1.0  26.56085  0.0  0.0  1396.000000      0\n",
       "...   ...   ...  ...       ...  ...  ...          ...    ...\n",
       "1608  0.0  71.0  0.0  23.05630  0.0  0.0  1464.000000      0\n",
       "1609  0.0  47.0  1.0  27.58821  1.0  0.0   608.000000      0\n",
       "1610  0.0  88.0  1.0  24.41255  0.0  1.0   732.945600      1\n",
       "1611  1.0  50.0  0.0  32.42784  0.0  0.0     4.904907      1\n",
       "1612  1.0  92.0  0.0  21.80987  0.0  1.0    34.919910      1\n",
       "1613  1.0  77.0  0.0  19.57790  0.0  0.0     0.660839      1\n",
       "1614  0.0  68.0  1.0  21.20757  0.0  0.0   545.000000      0\n",
       "1615  0.0  72.0  1.0  25.23266  0.0  0.0  1860.000000      0\n",
       "1616  1.0  86.0  1.0  19.85515  0.0  0.0   464.239100      1\n",
       "1617  0.0  80.0  1.0  22.31460  0.0  0.0  1225.000000      0\n",
       "1618  0.0  53.0  0.0  31.63544  0.0  1.0  1920.000000      0\n",
       "1619  0.0  67.0  0.0  27.40905  0.0  0.0   532.000000      0\n",
       "1620  0.0  95.0  1.0  19.04582  1.0  0.0  1668.995000      1\n",
       "1621  0.0  43.0  1.0  25.97231  0.0  1.0  1147.941000      1\n",
       "1622  0.0  72.0  1.0  25.23266  0.0  0.0  1860.000000      0\n",
       "1623  0.0  45.0  1.0  31.95113  0.0  1.0  1461.000000      0\n",
       "1624  1.0  76.0  0.0  22.43533  1.0  0.0  1117.189000      1\n",
       "1625  0.0  42.0  0.0  25.10507  1.0  1.0   989.476600      1\n",
       "1626  0.0  65.0  0.0  33.08808  1.0  1.0  1559.213000      1\n",
       "1627  0.0  73.0  0.0  24.21079  0.0  0.0  1904.000000      0\n",
       "1628  0.0  52.0  0.0  31.32463  0.0  1.0  1948.000000      0\n",
       "1629  0.0  78.0  1.0  27.53736  0.0  0.0  1232.000000      0\n",
       "1630  0.0  76.0  0.0  37.44985  0.0  1.0   585.973900      1\n",
       "1631  0.0  64.0  0.0  25.76880  0.0  0.0  1880.000000      0\n",
       "1632  0.0  63.0  0.0  28.41023  1.0  0.0   760.569300      1\n",
       "1633  0.0  77.0  0.0  29.83756  0.0  0.0  1874.000000      0\n",
       "1634  0.0  48.0  0.0  33.07924  0.0  1.0   556.000000      0\n",
       "1635  0.0  81.0  0.0  27.36909  0.0  1.0   511.000000      0\n",
       "1636  0.0  83.0  0.0  24.27603  0.0  0.0  1846.000000      0\n",
       "1637  0.0  77.0  1.0  23.08096  0.0  0.0  1845.000000      0\n",
       "\n",
       "[1638 rows x 8 columns]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnagpal/anaconda2/lib/python2.7/site-packages/lifelines/utils/__init__.py:900: ConvergenceWarning: Column 0 have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      ">>> events = df['fstat'].astype(bool)\n",
      ">>> df.loc[events, '0'].var()\n",
      ">>> df.loc[~events, '0'].var()\n",
      "\n",
      "Too low variance here means that the column 0 completely determines whether a subject dies or not.\n",
      "See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression \n",
      "  warnings.warn(warning_text, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 1638 observations, 948 censored>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoxPHFitter().fit(ds.iloc[], 'lenfol', 'fstat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = ds['fstat'].astype(bool)\n",
    "ds.loc[events, '0'].var()\n",
    "ds.loc[~events, '0'].var()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
