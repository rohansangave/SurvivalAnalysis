{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lifelines.utils import concordance_index \n",
    "import sys\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import network\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CHOICE = 4\n",
    "# WHAS = 1\n",
    "# GBSG = 2\n",
    "# METABRIC = 3\n",
    "# SUPPORT = 4\n",
    "\n",
    "if (DATASET_CHOICE == 1):\n",
    "    # WHAS\n",
    "    ds = pd.read_csv('./datasets/whas1638.csv',sep=',')\n",
    "    train = ds[:1310]\n",
    "    validation = ds[1310:]\n",
    "    x = train[['0','1', '2', '3', '4', '5']].as_matrix()\n",
    "    x_valid = validation[['0','1', '2', '3', '4', '5']].as_matrix() \n",
    "elif (DATASET_CHOICE == 2):\n",
    "    # GBSG\n",
    "    ds = pd.read_csv('./datasets/gbsg2232.csv',sep=',')\n",
    "    train = ds[:1546]\n",
    "    validation = ds[1546:]\n",
    "    x = train[['0','1', '2', '3', '4', '5', '6']].as_matrix()\n",
    "    x_valid = validation[['0','1', '2', '3', '4', '5', '6']].as_matrix() \n",
    "elif (DATASET_CHOICE == 3):\n",
    "    # for METABRIC\n",
    "    ds = pd.read_csv('./datasets/metabric1904.csv',sep=',')\n",
    "    train = ds[:1523]\n",
    "    validation = ds[1523:]\n",
    "    x = train[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix()\n",
    "    x_valid = validation[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix() \n",
    "elif (DATASET_CHOICE == 4):\n",
    "    # for SUPPORT\n",
    "    ds = pd.read_csv('./datasets/support8873.csv',sep=',')\n",
    "    train = ds[:7098]\n",
    "    validation = ds[7098:]\n",
    "    x = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix()\n",
    "    x_valid = validation[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "x = scl.fit_transform(x)\n",
    "\n",
    "e = train['fstat']\n",
    "t = train['lenfol']\n",
    "\n",
    "x_valid = scl.transform(x_valid)\n",
    "\n",
    "e_valid = validation['fstat']\n",
    "t_valid = validation['lenfol']\n",
    "\n",
    "x = torch.from_numpy(x).float()\n",
    "e = torch.from_numpy(e.as_matrix()).float()\n",
    "t = torch.from_numpy(t.as_matrix())\n",
    "\n",
    "x_valid = torch.from_numpy(x_valid).float()\n",
    "e_valid = torch.from_numpy(e_valid.as_matrix()).float()\n",
    "t_valid = torch.from_numpy(t_valid.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8873, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>lenfol</th>\n",
       "      <th>fstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.70996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>38.19531</td>\n",
       "      <td>142.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.099854</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.66095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>142.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.39999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.29688</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5.199219</td>\n",
       "      <td>1.199951</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.07599</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.00000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>8.699219</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>892.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.79498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.59375</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.099991</td>\n",
       "      <td>0.399963</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6      7      8     9        10  \\\n",
       "0  82.70996  1.0  2.0  1.0  0.0  0.0  0.0  160.0   55.0  16.0  38.19531   \n",
       "1  79.66095  1.0  0.0  1.0  0.0  0.0  1.0   54.0   67.0  16.0  38.00000   \n",
       "2  23.39999  1.0  2.0  3.0  0.0  0.0  1.0   87.0  144.0  45.0  37.29688   \n",
       "3  53.07599  1.0  4.0  3.0  0.0  0.0  0.0   55.0  100.0  18.0  36.00000   \n",
       "4  71.79498  0.0  1.0  1.0  0.0  0.0  0.0   65.0  135.0  40.0  38.59375   \n",
       "\n",
       "      11         12        13  lenfol  fstat  \n",
       "0  142.0  19.000000  1.099854    30.0      1  \n",
       "1  142.0  10.000000  0.899902  1527.0      0  \n",
       "2  130.0   5.199219  1.199951    96.0      1  \n",
       "3  135.0   8.699219  0.799927   892.0      0  \n",
       "4  146.0   0.099991  0.399963     7.0      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    x = x.cuda()\n",
    "    e = e.cuda()\n",
    "    t = t.cuda()\n",
    "    x_valid = x_valid.cuda()\n",
    "    e_valid = e_valid.cuda()\n",
    "    t_valid = t_valid.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  30., 1527.,   96.,  ...,  835.,  259.,  506.], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting risk set computation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987bb1406dce4d49b246f2119340895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7098), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19f439f636047cc90ee29f702e86131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1775), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "risk set computed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "#         m.weight.data.fill_(0)\n",
    "#         m.bias.data.fill_(1)\n",
    "\n",
    "def init_weights_for_cox(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "t_ = t.cpu().data.numpy()\n",
    "        \n",
    "print(\"starting risk set computation...\")\n",
    "risk_set = []\n",
    "for i in tqdm(range(len(t_))):\n",
    "\n",
    "    risk_set.append(np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "t_ = t_valid.cpu().data.numpy()\n",
    "\n",
    "risk_set_validation = []\n",
    "for i in tqdm(range(len(t_valid))):\n",
    "\n",
    "    risk_set_validation.append(np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "print(\"risk set computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def elbo(risk, gated_output, E, risk_set, CUDA):\n",
    "#     lgo_sm = nn.LogSoftmax(dim=1)(gated_output)\n",
    "#     lnumerator = torch.mul(torch.exp(lgo_sm), risk)\n",
    "    \n",
    "#     lnumerator = torch.sum(lnumerator, dim=1)\n",
    "    \n",
    "#     expected_risks = risk + lgo_sm\n",
    "#     expected_risks = torch.logsumexp(expected_risks, dim=1)\n",
    "#     ldenominator = []\n",
    "#     for i in range(risk.shape[0]):\n",
    "#         ldenominator.append(torch.logsumexp(expected_risks[risk_set[i]], dim=0))\n",
    "        \n",
    "#     ldenominator = torch.stack(ldenominator, dim=0)\n",
    "#    # ldenominator = torch.log(ldenominator)\n",
    "    \n",
    "#     likelihoods = lnumerator - ldenominator\n",
    "    \n",
    "#     E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "# #     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "#     likelihoods = likelihoods[E]\n",
    "#     neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "#     return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(risk, gated_output, E, risk_set, CUDA):\n",
    "    go_sm = nn.Softmax(dim=1)(gated_output)\n",
    "    lnumerator = torch.mul(go_sm, risk)\n",
    "    \n",
    "    lnumerator = torch.sum(lnumerator, dim=1)\n",
    "    \n",
    "    expected_risks = torch.exp(risk) * go_sm\n",
    "    expected_risks = torch.sum(expected_risks, dim=1)\n",
    "    ldenominator = []\n",
    "    for i in range(risk.shape[0]):\n",
    "        ldenominator.append(torch.sum(expected_risks[risk_set[i]], dim=0))\n",
    "        \n",
    "    ldenominator = torch.stack(ldenominator, dim=0)\n",
    "    ldenominator = torch.log(ldenominator)\n",
    "    \n",
    "    likelihoods = lnumerator - ldenominator\n",
    "    \n",
    "    E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "#     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "    likelihoods = likelihoods[E]\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gated_network, betas_network, risk_set, x, e, t, CUDA, optimizer, n_epochs,x_valid,e_valid,t_valid,risk_set_validation):\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    # Initialize Metrics\n",
    "    c_index = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    valid_c_index = []\n",
    "    diff = 1e-4\n",
    "    \n",
    "    prev_loss = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"x: \", x)\n",
    "        gated_outputs = gated_network(x)\n",
    "        lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "        \n",
    "        betas_output = betas_network(x)\n",
    "        \n",
    "        ci_train = get_concordance_index(betas_output, gated_outputs, t, e)\n",
    "        c_index.append(ci_train)\n",
    "        \n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss = elbo(betas_output, gated_outputs, e, risk_set, CUDA)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        my_loss = loss.cpu().data.numpy()\n",
    "        train_loss.append(my_loss)\n",
    "        if abs(my_loss - prev_loss) < diff:\n",
    "            break\n",
    "        prev_loss = my_loss\n",
    "        \n",
    "        \n",
    "        torch.cuda.empty_cache()          \n",
    "        #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "\n",
    "        gated_outputs_valid = gated_network(x_valid)\n",
    "        lsoftmax_valid = nn.LogSoftmax(dim=1)(gated_outputs_valid)\n",
    "        \n",
    "        betas_output_valid = betas_network(x_valid)\n",
    "        \n",
    "        ci_valid = get_concordance_index(betas_output_valid, gated_outputs_valid, t_valid, e_valid)\n",
    "        valid_c_index.append(ci_valid)\n",
    "        \n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss_valid = elbo(betas_output_valid, gated_outputs_valid, e_valid, risk_set_validation, CUDA)\n",
    "        \n",
    "\n",
    "        my_loss_valid = loss_valid.cpu().data.numpy()\n",
    "        valid_loss.append(my_loss_valid)\n",
    "        \n",
    "        \n",
    "        torch.cuda.empty_cache()          \n",
    "        #print('Finished Training with %d iterations in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "     \n",
    "        \n",
    "    metrics = {}\n",
    "    metrics['train_loss'] = train_loss\n",
    "    metrics['c-index'] = c_index\n",
    "    metrics['valid_loss'] = valid_loss\n",
    "    metrics['c-index-valid'] = valid_c_index\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(risk, lsoftmax, E, risk_set, CUDA):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "#     new_risk = []\n",
    "#     for i in range(len(risk_set)):\n",
    "#         new_risk.append(risk[risk_set[i]])\n",
    "        \n",
    "#     log_risk = []\n",
    "#     for i in range(len(new_risk)):\n",
    "#         temp = torch.logsumexp(new_risk[i], 0)\n",
    "#         log_risk.append(temp)\n",
    "\n",
    "    lnumerator = risk\n",
    "    \n",
    "    idxs = range(risk.shape[0])\n",
    "    \n",
    "    \n",
    "    ldenominator = []\n",
    "    \n",
    "    for i in range(len(idxs)):\n",
    "        ldenominator.append(torch.logsumexp(risk[risk_set[i]], dim=0))\n",
    "            \n",
    "                            \n",
    "    ldenominator = torch.stack(ldenominator, dim=0)\n",
    "    print(ldenominator.shape)\n",
    "#     print(lnumerator.shape)\n",
    "    \n",
    "    \n",
    "    likelihoods = lnumerator - ldenominator\n",
    "    print(likelihoods.shape)\n",
    "    \n",
    "    E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "#     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "    likelihoods = likelihoods[E] + lsoftmax[E]\n",
    "    likelihoods = torch.logsumexp(likelihoods, dim=1)\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "\n",
    "    return neg_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concordance_index(x, gated_x, t, e, **kwargs):\n",
    "#     x = x.detach().cpu().numpy()\n",
    "    t = t.detach().cpu().numpy()\n",
    "    e = e.detach().cpu().numpy()\n",
    "    softmax = torch.nn.Softmax(dim=1)(gated_x)\n",
    "    computed_hazard = torch.exp(x)\n",
    "\n",
    "    computed_hazard = torch.mul(softmax, computed_hazard)\n",
    "    computed_hazard = torch.sum(computed_hazard, dim = 1)\n",
    "    return concordance_index(t,-1*computed_hazard.detach().cpu().numpy(),e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cde2a58e1a4a0a9b253b5f67d17611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "()\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "n_in = x.shape[1]\n",
    "k = 2\n",
    "\n",
    "betas_network = nn.Sequential(nn.Linear(n_in, 1, bias=True))\n",
    "\n",
    "layers_sizes = [n_in, k]\n",
    "# Construct Neural Network\n",
    "layers = []\n",
    "for i in range(len(layers_sizes)-2):\n",
    "    layers.append(nn.Linear(layers_sizes[i],layers_sizes[i+1]))\n",
    "    layers.append(nn.ELU())\n",
    "\n",
    "layers.append(nn.Linear(layers_sizes[-2], layers_sizes[-1]))\n",
    "gated_network = nn.Sequential(*layers)\n",
    "#my_network.apply(init_weights)\n",
    "\n",
    "#optimizer = optimizer = torch.optim.SGD(my_network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=L2_reg, nesterov=True)\n",
    "optimizer = torch.optim.Adam(list(gated_network.parameters()) + list(betas_network.parameters()), lr=1e-3, weight_decay=0.01)\n",
    "betas_network.train()\n",
    "gated_network.train()\n",
    "\n",
    "if CUDA:\n",
    "    gated_network.cuda()\n",
    "    betas_network.cuda()\n",
    "\n",
    "# If you have validation data, you can add it as the valid_dataloader parameter to the function\n",
    "n_epochs = 2\n",
    "metrics = train(gated_network, betas_network, risk_set, x, e, t, CUDA, optimizer, n_epochs,x_valid,e_valid,t_valid,risk_set_validation)\n",
    "print() \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.551629987869\n",
      "0.569879792274\n"
     ]
    }
   ],
   "source": [
    "print(metrics['c-index'][-1])\n",
    "print(metrics['c-index-valid'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(5922.4765625, dtype=float32), array(5921.3837890625, dtype=float32)]\n",
      "[0.55132950078336862, 0.55162998786922501]\n",
      "[0.56973496368940479, 0.56987979227442953]\n"
     ]
    }
   ],
   "source": [
    "print(metrics['train_loss'])\n",
    "print(metrics['c-index'])\n",
    "print(metrics['c-index-valid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('num of epochs: ', 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the final metrics\n",
    "# print('Train C-Index:', metrics['c-index'])\n",
    "# print('Valid C-Index: ',metrics['valid_c-index'][-1])\n",
    "\n",
    "print(\"num of epochs: \", len(metrics['train_loss']))\n",
    "# print(metrics['train_loss'])\n",
    "# Plot the training / validation curves\n",
    "plt.plot(range(len(metrics['train_loss'])), metrics['train_loss'])\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('training loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(metrics['c-index'])), metrics['c-index'])\n",
    "plt.plot(range(len(metrics['c-index-valid'])), metrics['c-index-valid'])\n",
    "\n",
    "plt.xlabel('no. of epochs')\n",
    "plt.ylabel('concordance index')\n",
    "# plt.plot(range(len(metrics['c-index'])), metrics['c-index'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>lenfol</th>\n",
       "      <th>fstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.44662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.589239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.64232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.48160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1453.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.58821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.12942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.616980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.67519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.46412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.423138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.26457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1567.236000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.46070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1349.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.41255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278.901600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.48131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1965.911000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.27114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1929.816000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.22902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.52311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1335.580000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.84858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.80135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1236.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.54051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.264200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.62249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1884.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.05819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.97226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.84949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.89496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.839700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1423.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.53139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>722.746400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.63135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1163.169000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.19716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.45634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>617.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.42371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>509.371600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.56085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.05630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.58821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.41255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>732.945600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.42784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.904907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.80987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.919910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.57790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.20757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.23266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.85515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>464.239100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.31460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1225.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.63544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.40905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.04582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1668.995000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.97231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1147.941000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.23266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.95113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1461.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.43533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1117.189000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.10507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>989.476600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.08808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1559.213000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.21079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1904.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.32463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1948.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.53736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1232.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.44985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.973900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.76880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.41023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>760.569300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.83756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1874.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.07924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.36909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.27603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1846.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.08096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1845.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1638 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2         3    4    5       lenfol  fstat\n",
       "0     1.0  84.0  0.0  22.44662  1.0  1.0     7.589239      1\n",
       "1     0.0  36.0  0.0  31.64232  0.0  1.0  1266.000000      0\n",
       "2     0.0  76.0  0.0  46.48160  0.0  1.0  1453.000000      0\n",
       "3     0.0  47.0  1.0  27.58821  1.0  0.0   608.000000      0\n",
       "4     0.0  80.0  0.0  24.12942  1.0  0.0    22.616980      1\n",
       "5     0.0  77.0  0.0  23.67519  0.0  0.0  1313.000000      0\n",
       "6     0.0  90.0  1.0  27.46412  1.0  0.0     1.423138      1\n",
       "7     0.0  80.0  1.0  29.26457  1.0  0.0  1567.236000      1\n",
       "8     0.0  40.0  0.0  25.46070  0.0  0.0  1349.000000      0\n",
       "9     0.0  54.0  1.0  24.41255  1.0  1.0   278.901600      1\n",
       "10    0.0  81.0  0.0  28.48131  1.0  1.0  1965.911000      1\n",
       "11    0.0  75.0  0.0  29.27114  1.0  1.0  1929.816000      1\n",
       "12    0.0  63.0  0.0  39.22902  0.0  0.0  1999.000000      0\n",
       "13    0.0  79.0  0.0  24.52311  0.0  0.0  1335.580000      1\n",
       "14    0.0  80.0  0.0  25.84858  1.0  0.0   373.000000      0\n",
       "15    0.0  73.0  1.0  27.80135  0.0  0.0  1236.000000      0\n",
       "16    0.0  58.0  0.0  25.54051  0.0  0.0   162.264200      1\n",
       "17    0.0  73.0  1.0  28.62249  0.0  1.0  1884.000000      0\n",
       "18    0.0  58.0  0.0  31.05819  0.0  1.0  1114.000000      0\n",
       "19    0.0  80.0  1.0  17.97226  0.0  1.0   641.000000      0\n",
       "20    0.0  60.0  0.0  30.84949  0.0  0.0   469.000000      0\n",
       "21    1.0  86.0  1.0  18.89496  1.0  0.0   377.839700      1\n",
       "22    0.0  75.0  0.0  26.44901  0.0  1.0  1423.000000      0\n",
       "23    0.0  77.0  1.0  28.53139  1.0  0.0   722.746400      1\n",
       "24    0.0  41.0  0.0  26.44693  0.0  0.0  1988.000000      0\n",
       "25    0.0  90.0  1.0  23.63135  0.0  0.0  1163.169000      1\n",
       "26    0.0  58.0  0.0  27.19716  0.0  0.0   412.000000      0\n",
       "27    0.0  79.0  1.0  21.45634  0.0  0.0   617.000000      0\n",
       "28    0.0  65.0  1.0  49.42371  1.0  0.0   509.371600      1\n",
       "29    0.0  75.0  1.0  26.56085  0.0  0.0  1396.000000      0\n",
       "...   ...   ...  ...       ...  ...  ...          ...    ...\n",
       "1608  0.0  71.0  0.0  23.05630  0.0  0.0  1464.000000      0\n",
       "1609  0.0  47.0  1.0  27.58821  1.0  0.0   608.000000      0\n",
       "1610  0.0  88.0  1.0  24.41255  0.0  1.0   732.945600      1\n",
       "1611  1.0  50.0  0.0  32.42784  0.0  0.0     4.904907      1\n",
       "1612  1.0  92.0  0.0  21.80987  0.0  1.0    34.919910      1\n",
       "1613  1.0  77.0  0.0  19.57790  0.0  0.0     0.660839      1\n",
       "1614  0.0  68.0  1.0  21.20757  0.0  0.0   545.000000      0\n",
       "1615  0.0  72.0  1.0  25.23266  0.0  0.0  1860.000000      0\n",
       "1616  1.0  86.0  1.0  19.85515  0.0  0.0   464.239100      1\n",
       "1617  0.0  80.0  1.0  22.31460  0.0  0.0  1225.000000      0\n",
       "1618  0.0  53.0  0.0  31.63544  0.0  1.0  1920.000000      0\n",
       "1619  0.0  67.0  0.0  27.40905  0.0  0.0   532.000000      0\n",
       "1620  0.0  95.0  1.0  19.04582  1.0  0.0  1668.995000      1\n",
       "1621  0.0  43.0  1.0  25.97231  0.0  1.0  1147.941000      1\n",
       "1622  0.0  72.0  1.0  25.23266  0.0  0.0  1860.000000      0\n",
       "1623  0.0  45.0  1.0  31.95113  0.0  1.0  1461.000000      0\n",
       "1624  1.0  76.0  0.0  22.43533  1.0  0.0  1117.189000      1\n",
       "1625  0.0  42.0  0.0  25.10507  1.0  1.0   989.476600      1\n",
       "1626  0.0  65.0  0.0  33.08808  1.0  1.0  1559.213000      1\n",
       "1627  0.0  73.0  0.0  24.21079  0.0  0.0  1904.000000      0\n",
       "1628  0.0  52.0  0.0  31.32463  0.0  1.0  1948.000000      0\n",
       "1629  0.0  78.0  1.0  27.53736  0.0  0.0  1232.000000      0\n",
       "1630  0.0  76.0  0.0  37.44985  0.0  1.0   585.973900      1\n",
       "1631  0.0  64.0  0.0  25.76880  0.0  0.0  1880.000000      0\n",
       "1632  0.0  63.0  0.0  28.41023  1.0  0.0   760.569300      1\n",
       "1633  0.0  77.0  0.0  29.83756  0.0  0.0  1874.000000      0\n",
       "1634  0.0  48.0  0.0  33.07924  0.0  1.0   556.000000      0\n",
       "1635  0.0  81.0  0.0  27.36909  0.0  1.0   511.000000      0\n",
       "1636  0.0  83.0  0.0  24.27603  0.0  0.0  1846.000000      0\n",
       "1637  0.0  77.0  1.0  23.08096  0.0  0.0  1845.000000      0\n",
       "\n",
       "[1638 rows x 8 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnagpal/anaconda2/lib/python2.7/site-packages/lifelines/utils/__init__.py:900: ConvergenceWarning: Column 0 have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      ">>> events = df['fstat'].astype(bool)\n",
      ">>> df.loc[events, '0'].var()\n",
      ">>> df.loc[~events, '0'].var()\n",
      "\n",
      "Too low variance here means that the column 0 completely determines whether a subject dies or not.\n",
      "See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression \n",
      "  warnings.warn(warning_text, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 1638 observations, 948 censored>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoxPHFitter().fit(ds.iloc[], 'lenfol', 'fstat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = ds['fstat'].astype(bool)\n",
    "ds.loc[events, '0'].var()\n",
    "ds.loc[~events, '0'].var()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
